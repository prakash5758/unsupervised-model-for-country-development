{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbb2bc86-f714-4ab7-9f7e-946cdb770a59",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ae57e37-163a-4b06-b07c-2aa4e8e8b81d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+h3_hint": "",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DateType, IntegerType\n",
    "from datetime import datetime, date\n",
    "from pyspark.sql.functions import col, desc\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import shapely.wkt\n",
    "import functools as ft\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76cb1dc2-c25e-4341-924d-87e783c69270",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Input Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "405c230d-9924-4014-b8b5-aebbca52fa24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# basin_of_interest = 'GULF COAST EAST'\n",
    "# cutoff_date = '2019-04-01'\n",
    "# flowunit_of_interest = 'HAYNESVILLE'\n",
    "# scenario_id = \"1\"\n",
    "# current_date = \"2024-04-29\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b6347bc-e6de-463c-ad24-c71585b9fd59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1395104717220172>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m basin_of_interest \u001B[38;5;241m=\u001B[39m dbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbasin_of_interest\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      2\u001B[0m cutoff_date \u001B[38;5;241m=\u001B[39m dbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcutoff_date_for_training_data\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      3\u001B[0m flowunit_of_interest \u001B[38;5;241m=\u001B[39m dbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mflow_unit_of_interest\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/WidgetHandlerImpl.py:43\u001B[0m, in \u001B[0;36mWidgetsHandlerImpl.get\u001B[0;34m(self, name)\u001B[0m\n",
       "\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(\u001B[38;5;28mself\u001B[39m, name):\n",
       "\u001B[1;32m     38\u001B[0m     \u001B[38;5;124;03m\"\"\" Returns the current value of a widget with give name.\u001B[39;00m\n",
       "\u001B[1;32m     39\u001B[0m \n",
       "\u001B[1;32m     40\u001B[0m \u001B[38;5;124;03m    :param name: Name of the argument to be accessed\u001B[39;00m\n",
       "\u001B[1;32m     41\u001B[0m \u001B[38;5;124;03m    :return: Current value of the widget or default value\u001B[39;00m\n",
       "\u001B[1;32m     42\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m---> 43\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_notebookArguments\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetArgument\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_entry_point\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetCurrentBindings\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:224\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    222\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m    223\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 224\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    225\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    226\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o397.getArgument.\n",
       ": com.databricks.dbutils_v1.InputWidgetNotDefined: No input widget named basin_of_interest is defined\n",
       "\tat com.databricks.backend.daemon.driver.NotebookArguments.checkExists(NotebookArguments.scala:72)\n",
       "\tat com.databricks.backend.daemon.driver.NotebookArguments.getArgument(NotebookArguments.scala:264)\n",
       "\tat sun.reflect.GeneratedMethodAccessor420.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "Py4JJavaError",
        "evalue": "An error occurred while calling o397.getArgument.\n: com.databricks.dbutils_v1.InputWidgetNotDefined: No input widget named basin_of_interest is defined\n\tat com.databricks.backend.daemon.driver.NotebookArguments.checkExists(NotebookArguments.scala:72)\n\tat com.databricks.backend.daemon.driver.NotebookArguments.getArgument(NotebookArguments.scala:264)\n\tat sun.reflect.GeneratedMethodAccessor420.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\n"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>Py4JJavaError</span>: An error occurred while calling o397.getArgument.\n: com.databricks.dbutils_v1.InputWidgetNotDefined: No input widget named basin_of_interest is defined\n\tat com.databricks.backend.daemon.driver.NotebookArguments.checkExists(NotebookArguments.scala:72)\n\tat com.databricks.backend.daemon.driver.NotebookArguments.getArgument(NotebookArguments.scala:264)\n\tat sun.reflect.GeneratedMethodAccessor420.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\n"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
        "File \u001B[0;32m<command-1395104717220172>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m basin_of_interest \u001B[38;5;241m=\u001B[39m dbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbasin_of_interest\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      2\u001B[0m cutoff_date \u001B[38;5;241m=\u001B[39m dbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcutoff_date_for_training_data\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      3\u001B[0m flowunit_of_interest \u001B[38;5;241m=\u001B[39m dbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mflow_unit_of_interest\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python_shell/dbruntime/WidgetHandlerImpl.py:43\u001B[0m, in \u001B[0;36mWidgetsHandlerImpl.get\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(\u001B[38;5;28mself\u001B[39m, name):\n\u001B[1;32m     38\u001B[0m     \u001B[38;5;124;03m\"\"\" Returns the current value of a widget with give name.\u001B[39;00m\n\u001B[1;32m     39\u001B[0m \n\u001B[1;32m     40\u001B[0m \u001B[38;5;124;03m    :param name: Name of the argument to be accessed\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;124;03m    :return: Current value of the widget or default value\u001B[39;00m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 43\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_notebookArguments\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetArgument\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_entry_point\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetCurrentBindings\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:224\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    223\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 224\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    225\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    226\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
        "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o397.getArgument.\n: com.databricks.dbutils_v1.InputWidgetNotDefined: No input widget named basin_of_interest is defined\n\tat com.databricks.backend.daemon.driver.NotebookArguments.checkExists(NotebookArguments.scala:72)\n\tat com.databricks.backend.daemon.driver.NotebookArguments.getArgument(NotebookArguments.scala:264)\n\tat sun.reflect.GeneratedMethodAccessor420.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "basin_of_interest = dbutils.widgets.get(\"basin_of_interest\")\n",
    "cutoff_date = dbutils.widgets.get(\"cutoff_date_for_training_data\")\n",
    "flowunit_of_interest = dbutils.widgets.get(\"flow_unit_of_interest\")\n",
    "scenario_id = dbutils.widgets.get(\"scenario_id\")\n",
    "current_date = dbutils.widgets.get(\"current_date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76611de3-26e8-4262-ae6e-78d1571906c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# TypeCurve dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aacdbdb0-0bd7-40f3-9076-2d4d6d5fbb72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "typecurve_df = spark.sql(\n",
    "    f\"\"\"\n",
    "    SELECT\n",
    "        API10,\n",
    "        typeCurveArea,\n",
    "        FlowUnit_Analog\n",
    "    FROM\n",
    "        produced.analog_well_selection\n",
    "    WHERE\n",
    "        recentWell = \"true\"\n",
    "        AND flowUnit_Analog = '{flowunit_of_interest}'\n",
    "\"\"\"\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0802812e-190f-4d86-8623-ae983bf3859e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class Downloader:\n",
    "    \"\"\"\n",
    "    A class for downloading well, TCA, and economics data using PySpark.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, well_data_table: str, analog_well_table: str, basin_of_interest: str\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the Downloader object with necessary parameters.\n",
    "\n",
    "        Parameters:\n",
    "        - well_data_table (str): Table name for well data.\n",
    "        - flowunit_of_interest (str): flow unit of interest.\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        self.well_data_table = well_data_table\n",
    "        self.analog_well_table = analog_well_table\n",
    "        self.flowunit_of_interest = flowunit_of_interest\n",
    "\n",
    "    def download_well_data(self, filter_date, current_date):\n",
    "        query = f\"\"\"\n",
    "        SELECT\n",
    "        *\n",
    "        EXCEPT(LateralLength_FT, fu_median_ll, tca_median_ll),\n",
    "        COALESCE(LateralLength_FT, tca_median_ll, fu_median_ll) AS LateralLength_FT\n",
    "        FROM (\n",
    "            SELECT\n",
    "            ana.API10, ana.API14, ana.LateralLength_FT, ana.typeCurveArea, ana.BasinQuantum, ana.FlowUnit_Analog, com.CompletionDate, ana.OperatorGold, ana.FirstProdDate, com.EnvPermitSubmittedDate, com.PermitApprovedDate, ana.SpudDate, com.RigReleaseDate,\n",
    "            PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY ana.LateralLength_FT) OVER () AS fu_median_ll,\n",
    "            PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY ana.LateralLength_FT) OVER (PARTITION BY typeCurveArea) AS tca_median_ll\n",
    "            FROM {self.well_data_table} com\n",
    "            INNER JOIN {self.analog_well_table} ana\n",
    "            ON ana.API14 = com.API14\n",
    "            AND ana.recentWell = 'true'\n",
    "            AND ana.FlowUnit_Analog = '{self.flowunit_of_interest}'\n",
    "            AND ana.FirstProdDate < '{current_date}'\n",
    "            AND ana.FirstProdDate > '{filter_date}'\n",
    "        ) AS subquery\n",
    "        \"\"\"\n",
    "\n",
    "        df = spark.sql(query).toPandas()\n",
    "\n",
    "        df = df.loc[df.groupby(\"API10\")[\"LateralLength_FT\"].idxmax()]\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b85daa23-ebe4-4fba-86c8-7a95b00d97c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "well_completion_table = \"produced.vw_well_completions_merged\"\n",
    "analog_well_table = \"produced.analog_well_selection\"\n",
    "filter_date_for_training = cutoff_date\n",
    "\n",
    "download = Downloader(well_completion_table, analog_well_table, flowunit_of_interest)\n",
    "final_df = download.download_well_data(filter_date_for_training, current_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d06afa7-4f10-4581-b93d-b735e36a68a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Spud to rig release (historical data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dbf5fd9-557b-4fe6-88f8-0e4944817df1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class RigsHistorical:\n",
    "    def __init__(\n",
    "        self, rig_historical_table: str, rig_historical_col: list, flow_unit_of_interest\n",
    "    ):\n",
    "\n",
    "        self.rig_historical_table = rig_historical_table\n",
    "        self.analog_well_table = analog_well_table\n",
    "        self.flow_unit_of_interest = flow_unit_of_interest\n",
    "\n",
    "    def download_historical_rig_data(self, cutoff_date, current_date) -> pd.DataFrame:\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT\n",
    "            date, com.API10, com.operator, com.reservoir_gold_consolidated, ana.typeCurveArea, com.BasinQuantum, ana.FlowUnit_Analog, rig_id\n",
    "            FROM\n",
    "            {self.rig_historical_table} com\n",
    "            INNER JOIN\n",
    "            {self.analog_well_table} ana\n",
    "            ON\n",
    "            ana.api10 = com.api10\n",
    "            AND\n",
    "            ana.recentWell = 'true'\n",
    "            AND ana.FlowUnit_Analog = '{self.flow_unit_of_interest}'\n",
    "            AND ana.FirstProdDate < '{current_date}'\n",
    "            AND ana.FirstProdDate > '{cutoff_date}'\n",
    "\n",
    "        \"\"\"\n",
    "        df = spark.sql(query).toPandas()\n",
    "        df.rename(\n",
    "            {\"operator\": \"OperatorGold\", \"date\": \"time_taken_spud_to_rigrelease\"},\n",
    "            inplace=True,\n",
    "            axis=1,\n",
    "        )\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d68dfb47-4a66-4225-a0fd-7ac8f9a0f21a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rigs_historical_data_table = \"produced.private_rigs_history\"\n",
    "api = tuple(final_df.API10.unique())\n",
    "righistorical_download = RigsHistorical(\n",
    "    rigs_historical_data_table, analog_well_table, flowunit_of_interest\n",
    ")\n",
    "rig_history_df = righistorical_download.download_historical_rig_data(\n",
    "    cutoff_date, current_date\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "547afcdd-9911-440f-9757-93cd3690a1c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rig_time_df = rig_history_df.groupby([\"API10\"], as_index=False)[\n",
    "    \"time_taken_spud_to_rigrelease\"\n",
    "].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad939ff3-9d31-4a6d-bce4-b03b63a205b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rig_time_df[\"time_taken_spud_to_rigrelease\"] = pd.to_timedelta(\n",
    "    rig_time_df[\"time_taken_spud_to_rigrelease\"], unit=\"D\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b56dbaf2-8f11-424d-9eb5-12bc7717e784",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    \"\"\"\n",
    "    Preprocesses the input DataFrame by filling null values, selecting rows based on specific criteria,\n",
    "    and creating new time-related columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fill null values in the 'LateralLength_FT' column with -100\n",
    "    df[\"LateralLength_FT\"] = df[\"LateralLength_FT\"].fillna(-100)\n",
    "\n",
    "    # Select API10 entries with the greatest 'LateralLength_FT' for wells that possess more than one API14\n",
    "    max_values = df.groupby(\"API10\")[\"LateralLength_FT\"].idxmax()\n",
    "    df = df.loc[max_values]\n",
    "\n",
    "    # Replace 0 values with None in 'LateralLength_FT' column\n",
    "    df[\"LateralLength_FT\"] = df[\"LateralLength_FT\"].replace(0, None)\n",
    "\n",
    "    # Calculate time taken for different phases\n",
    "    df[\"time_taken_premit_submit_to_appr\"] = (\n",
    "        df[\"PermitApprovedDate\"] - df[\"EnvPermitSubmittedDate\"]\n",
    "    )\n",
    "    df[\"time_taken_premit_appr_to_spud\"] = df[\"SpudDate\"] - df[\"PermitApprovedDate\"]\n",
    "    df[\"time_taken_spud_to_completion\"] = df[\"CompletionDate\"] - df[\"SpudDate\"]\n",
    "    df[\"time_taken_completion_to_firstprod\"] = (\n",
    "        df[\"FirstProdDate\"] - df[\"CompletionDate\"]\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a076956-36b4-4604-9d8e-51fd679792f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df[\"FirstProdDate\"] = pd.to_datetime(final_df[\"FirstProdDate\"])\n",
    "final_df[\"SpudDate\"] = pd.to_datetime(final_df[\"SpudDate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a5b4e5b-92ea-4281-8707-28b4801ed53e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = preprocessing(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9dbff81-2b1f-4662-9037-00ec5bba8996",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = pd.merge(final_df, rig_time_df, on=\"API10\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13062374-8165-4152-b2a7-cf1bdba6341d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = final_df.dropna(\n",
    "    subset=[\"time_taken_spud_to_rigrelease\", \"time_taken_spud_to_completion\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8742fe95-038f-45b0-ad25-aadfdfa1eb75",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Cycle time calculation Opr and TCA level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4c1dabe-d3e6-430c-b167-b7be43b9e420",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_threshold_limit(df, operator, typecurve, threshold=9):\n",
    "    \"\"\"\n",
    "    Checks if the number of wells for the given operator and typecurve exceeds a threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "    - operator (str): Operator name.\n",
    "    - typecurve (str): Typecurve area.\n",
    "    - threshold (int): Threshold limit (default is 9).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Filtered DataFrame based on the threshold.\n",
    "    \"\"\"\n",
    "    filtered_df = df[\n",
    "        (df[\"OperatorGold\"] == operator) & (df[\"typeCurveArea\"] == typecurve)\n",
    "    ]\n",
    "\n",
    "    if len(filtered_df) >= threshold:\n",
    "        return filtered_df\n",
    "    elif len(df[df[\"OperatorGold\"] == operator]) >= threshold:\n",
    "        return df[df[\"OperatorGold\"] == operator]\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "\n",
    "def get_time_taken_premit_submit_to_appr(df, whole_basin_df):\n",
    "    \"\"\"\n",
    "    Calculates the median time taken from permit submission to approval.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - float: Median time taken.\n",
    "    \"\"\"\n",
    "    median = df[\"time_taken_premit_submit_to_appr\"].median()\n",
    "    if median:\n",
    "        return median\n",
    "    else:\n",
    "        whole_basin_df[\"time_taken_premit_submit_to_appr\"].median()\n",
    "\n",
    "\n",
    "def get_time_taken_premit_appr_to_spud(df, whole_basin_df):\n",
    "    \"\"\"\n",
    "    Calculates the median time taken from permit approval to spud.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - float: Median time taken.\n",
    "    \"\"\"\n",
    "    median = df[\"time_taken_premit_appr_to_spud\"].median()\n",
    "    if median:\n",
    "        return median\n",
    "    else:\n",
    "        whole_basin_df[\"time_taken_premit_appr_to_spud\"].median()\n",
    "\n",
    "\n",
    "def get_time_taken_spud_to_completion(df, whole_basin_df):\n",
    "    \"\"\"\n",
    "    Calculates the median time taken from spud to completion.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - float: Median time taken.\n",
    "    \"\"\"\n",
    "    median = df[\"time_taken_spud_to_completion\"].median()\n",
    "    if median:\n",
    "        return median\n",
    "    else:\n",
    "        whole_basin_df[\"time_taken_spud_to_completion\"].median()\n",
    "\n",
    "\n",
    "def get_time_taken_completion_to_firstprod(df, whole_basin_df):\n",
    "    \"\"\"\n",
    "    Calculates the median time taken from completion to first production.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - float: Median time taken.\n",
    "    \"\"\"\n",
    "    median = df[\"time_taken_completion_to_firstprod\"].median()\n",
    "    if median:\n",
    "        return median\n",
    "    else:\n",
    "        whole_basin_df[\"time_taken_completion_to_firstprod\"].median()\n",
    "\n",
    "\n",
    "def get_time_taken_spud_to_rigrelease(df, whole_basin_df):\n",
    "    \"\"\"\n",
    "    Calculates the median time taken from spud to rig release.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - float: Median time taken.\n",
    "    \"\"\"\n",
    "    median = df[\"time_taken_spud_to_rigrelease\"].median()\n",
    "    if median is not None:\n",
    "        return median\n",
    "    else:\n",
    "        print(\"here----\")\n",
    "        whole_basin_df[\"time_taken_spud_to_rigrelease\"].median()\n",
    "\n",
    "\n",
    "def get_cycle_times(operator, typecurve, df, whole_basin_df):\n",
    "    \"\"\"\n",
    "    Gets median times for different phases of well cycle.\n",
    "\n",
    "    Parameters:\n",
    "    - operator (str): Operator name.\n",
    "    - typecurve (str): Typecurve area.\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of median times.\n",
    "    \"\"\"\n",
    "    new_df = check_threshold_limit(df, operator, typecurve)\n",
    "    time_taken_premit_submit_to_appr = get_time_taken_premit_submit_to_appr(new_df, df)\n",
    "    time_taken_premit_appr_to_spud = get_time_taken_premit_appr_to_spud(new_df, df)\n",
    "    time_taken_spud_to_rigrelease = get_time_taken_spud_to_rigrelease(new_df, df)\n",
    "    time_taken_spud_to_completion = get_time_taken_spud_to_completion(new_df, df)\n",
    "    time_taken_completion_to_firstprod = get_time_taken_completion_to_firstprod(\n",
    "        new_df, df\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        time_taken_premit_submit_to_appr,\n",
    "        time_taken_premit_appr_to_spud,\n",
    "        time_taken_spud_to_rigrelease,\n",
    "        time_taken_spud_to_completion,\n",
    "        time_taken_completion_to_firstprod,\n",
    "    ]\n",
    "\n",
    "\n",
    "def cycle_time_calculation(df, df2):\n",
    "    \"\"\"\n",
    "    Calculates cycle times for different operators and typecurves.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with calculated cycle times.\n",
    "    \"\"\"\n",
    "    opr_tca_df = df.groupby([\"OperatorGold\", \"typeCurveArea\"], as_index=False)[\n",
    "        \"API10\"\n",
    "    ].count()\n",
    "    opr_tca_df.rename({\"API10\": \"num_of_wells\"}, inplace=True, axis=1)\n",
    "    opr_tca_df[\n",
    "        [\n",
    "            \"time_taken_premit_submit_to_appr\",\n",
    "            \"time_taken_premit_appr_to_spud\",\n",
    "            \"time_taken_spud_to_rigrelease\",\n",
    "            \"time_taken_spud_to_completion\",\n",
    "            \"time_taken_completion_to_firstprod\",\n",
    "        ]\n",
    "    ] = opr_tca_df.apply(\n",
    "        lambda row: pd.Series(\n",
    "            get_cycle_times(row[\"OperatorGold\"], row[\"typeCurveArea\"], df, df2)\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    opr_tca_df[\"time_taken_spud_to_rigrelease\"] = opr_tca_df[\n",
    "        \"time_taken_spud_to_rigrelease\"\n",
    "    ].fillna(opr_tca_df[\"time_taken_spud_to_rigrelease\"].median())\n",
    "\n",
    "    opr_tca_df[\"time_taken_completion_to_firstprod\"] = opr_tca_df[\n",
    "        \"time_taken_completion_to_firstprod\"\n",
    "    ].fillna(opr_tca_df[\"time_taken_completion_to_firstprod\"].median())\n",
    "\n",
    "    opr_tca_df[\"time_taken_spud_to_completion\"] = opr_tca_df[\n",
    "        \"time_taken_spud_to_completion\"\n",
    "    ].fillna(opr_tca_df[\"time_taken_spud_to_completion\"].median())\n",
    "    return opr_tca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a849524a-2ae6-442d-8c0d-8ba2ec9fb89d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df_copy = final_df.copy(deep=True)\n",
    "opr_tca_df = cycle_time_calculation(final_df, final_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e39cf34-7cda-43a6-a326-af5819109b9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "opr_tca_df[\"BasinQuantum\"] = basin_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96531acf-8e8a-46f3-95be-1d0766e10540",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "opr_tca_df[\"time_taken_premit_submit_to_appr\"] = opr_tca_df[\n",
    "    \"time_taken_premit_submit_to_appr\"\n",
    "].dt.days\n",
    "opr_tca_df[\"time_taken_premit_appr_to_spud\"] = opr_tca_df[\n",
    "    \"time_taken_premit_appr_to_spud\"\n",
    "].dt.days\n",
    "opr_tca_df[\"time_taken_spud_to_rigrelease\"] = opr_tca_df[\n",
    "    \"time_taken_spud_to_rigrelease\"\n",
    "].dt.days\n",
    "opr_tca_df[\"time_taken_spud_to_completion\"] = opr_tca_df[\n",
    "    \"time_taken_spud_to_completion\"\n",
    "].dt.days\n",
    "opr_tca_df[\"time_taken_completion_to_firstprod\"] = opr_tca_df[\n",
    "    \"time_taken_completion_to_firstprod\"\n",
    "].dt.days\n",
    "opr_tca_df[\"scenario_id\"] = scenario_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0ae08e1-bcb7-4ce1-ba08-7badf240c49b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = final_df[\n",
    "    [\n",
    "        \"API10\",\n",
    "        \"OperatorGold\",\n",
    "        \"typeCurveArea\",\n",
    "        \"time_taken_premit_submit_to_appr\",\n",
    "        \"time_taken_premit_appr_to_spud\",\n",
    "        \"time_taken_spud_to_rigrelease\",\n",
    "        \"time_taken_spud_to_completion\",\n",
    "        \"time_taken_completion_to_firstprod\",\n",
    "        \"BasinQuantum\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8998e9c-bc11-4087-bcbc-5067196f0a8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df[\"time_taken_premit_submit_to_appr\"] = final_df[\n",
    "    \"time_taken_premit_submit_to_appr\"\n",
    "].dt.days\n",
    "final_df[\"time_taken_premit_appr_to_spud\"] = final_df[\n",
    "    \"time_taken_premit_appr_to_spud\"\n",
    "].dt.days\n",
    "final_df[\"time_taken_spud_to_rigrelease\"] = final_df[\n",
    "    \"time_taken_spud_to_rigrelease\"\n",
    "].dt.days\n",
    "final_df[\"time_taken_spud_to_completion\"] = final_df[\n",
    "    \"time_taken_spud_to_completion\"\n",
    "].dt.days\n",
    "final_df[\"time_taken_completion_to_firstprod\"] = final_df[\n",
    "    \"time_taken_completion_to_firstprod\"\n",
    "].dt.days\n",
    "final_df[\"scenario_id\"] = scenario_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0838631-fe96-427f-856b-f83307afc56b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Creating Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f04e745-2eb5-47bb-856f-5edb65908122",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "          delete from produced.api_level_cycle_times where scenario_id = \"{scenario_id}\"\n",
    "          \"\"\"\n",
    ")\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "          delete from produced.operator_cycle_times where scenario_id = \"{scenario_id}\"\n",
    "          \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0338736d-eb5c-4e13-820b-baf5ccda8003",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    spark.createDataFrame(final_df)\n",
    "    .withColumn(\n",
    "        \"time_taken_premit_submit_to_appr\",\n",
    "        col(\"time_taken_premit_submit_to_appr\").cast(IntegerType()),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"time_taken_premit_appr_to_spud\",\n",
    "        col(\"time_taken_premit_appr_to_spud\").cast(IntegerType()),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"time_taken_spud_to_rigrelease\",\n",
    "        col(\"time_taken_spud_to_rigrelease\").cast(IntegerType()),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"time_taken_spud_to_completion\",\n",
    "        col(\"time_taken_spud_to_completion\").cast(IntegerType()),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"time_taken_completion_to_firstprod\",\n",
    "        col(\"time_taken_completion_to_firstprod\").cast(IntegerType()),\n",
    "    )\n",
    "    .write.format(\"delta\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .mode(\"append\")\n",
    "    .saveAsTable(f\"produced.api_level_cycle_times\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b54e4bd-a535-4caa-9558-0636bff82aef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    spark.createDataFrame(opr_tca_df)\n",
    "    .withColumn(\"num_of_wells\", col(\"num_of_wells\").cast(IntegerType()))\n",
    "    .withColumn(\n",
    "        \"time_taken_premit_submit_to_appr\",\n",
    "        col(\"time_taken_premit_submit_to_appr\").cast(IntegerType()),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"time_taken_premit_appr_to_spud\",\n",
    "        col(\"time_taken_premit_appr_to_spud\").cast(IntegerType()),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"time_taken_spud_to_rigrelease\",\n",
    "        col(\"time_taken_spud_to_rigrelease\").cast(IntegerType()),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"time_taken_spud_to_completion\",\n",
    "        col(\"time_taken_spud_to_completion\").cast(IntegerType()),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"time_taken_completion_to_firstprod\",\n",
    "        col(\"time_taken_completion_to_firstprod\").cast(IntegerType()),\n",
    "    )\n",
    "    .write.format(\"delta\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .mode(\"append\")\n",
    "    .saveAsTable(f\"produced.operator_cycle_times\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1395104717220213,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01-CYCLE-TIME-MODEL",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
