{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cd979d5-8f6d-4179-9fa6-ba18e7ae9bde",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4df8222e-394d-46d8-8e39-779f3e3609de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+h3_hint": "",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import col\n",
    "from datetime import date\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import shapely.wkt\n",
    "import functools as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ead9362-bac9-4efd-ac4f-5ccd71242323",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Input Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6202518a-6268-4439-b217-d74cbe697649",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# user_input_num_of_rigs = 35\n",
    "# basin_of_interest = 'GULF COAST EAST'\n",
    "# flowunit_of_interest = 'HAYNESVILLE'\n",
    "# desired_active_rig_date = \"2023-10-01\"\n",
    "# training_cutoff_date = \"2022-04-01\"\n",
    "# cutoff_misc_perc = 5\n",
    "# scenario_id = \"1\"\n",
    "# current_date = \"2024-04-29\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9487c30-20e0-4472-968b-48d89519a031",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_input_num_of_rigs = dbutils.widgets.get(\"user_input_for_num_of_rigs\")\n",
    "basin_of_interest = dbutils.widgets.get(\"basin_of_interest\")\n",
    "flowunit_of_interest = dbutils.widgets.get(\"flow_unit_of_interest\")\n",
    "desired_active_rig_date = dbutils.widgets.get(\"desired_active_rig_date\")\n",
    "training_cutoff_date = dbutils.widgets.get(\"cutoff_date_to_train_rig_model\")\n",
    "cutoff_misc_perc = int(dbutils.widgets.get(\"cutoff_perc_for_misc_operators\"))\n",
    "current_date = dbutils.widgets.get(\"current_date\")\n",
    "scenario_id = dbutils.widgets.get(\"scenario_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13cacf55-0c92-467e-b837-74ae8d9ee9c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Rig Data Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "045361ec-93b8-47f3-b296-9a97d7a0a471",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class RigsHistorical:\n",
    "    def __init__(\n",
    "        self,\n",
    "        rig_historical_table: str,\n",
    "        rig_historical_col: list,\n",
    "        flow_unit_of_interest,\n",
    "        basin_of_interest,\n",
    "    ):\n",
    "\n",
    "        self.rig_historical_table = rig_historical_table\n",
    "        self.analog_well_table = analog_well_table\n",
    "        self.flow_unit_of_interest = flow_unit_of_interest\n",
    "        self.basin_of_interest = basin_of_interest\n",
    "\n",
    "    def download_historical_rig_data(self, desired_active_rig_date) -> pd.DataFrame:\n",
    "\n",
    "        query = f\"\"\"\n",
    "        SELECT\n",
    "            date, com.api10, SpudDate, com.operator, com.reservoir_gold_consolidated, \n",
    "            ana.typeCurveArea, com.BasinQuantum, ana.FlowUnit_Analog, ana.LateralLength_FT, rig_id\n",
    "        FROM\n",
    "            {self.rig_historical_table} AS com\n",
    "        INNER JOIN\n",
    "            {self.analog_well_table} AS ana\n",
    "        ON\n",
    "            ana.api10 = com.api10\n",
    "            AND ana.recentWell = 'true'\n",
    "            AND com.BasinQuantum = '{self.basin_of_interest}'\n",
    "            AND ana.FlowUnit_Analog = '{self.flow_unit_of_interest}'\n",
    "            AND date = '{desired_active_rig_date}'\n",
    "        \"\"\"\n",
    "\n",
    "        df = spark.sql(query).toPandas()\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3226eed2-41a5-438d-a57c-64c672b7f2c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/pandas/utils.py:109: UserWarning: The conversion of DecimalType columns is inefficient and may take a long time. Column names: [LateralLength_FT] If those columns are not necessary, you may consider dropping them or converting to primitive types before the conversion.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "rigs_historical_data_table = \"produced.private_rigs_history\"\n",
    "analog_well_table = \"produced.analog_well_selection\"\n",
    "righistorical_download = RigsHistorical(\n",
    "    rigs_historical_data_table,\n",
    "    analog_well_table,\n",
    "    flowunit_of_interest,\n",
    "    basin_of_interest,\n",
    ")\n",
    "rig_data = righistorical_download.download_historical_rig_data(desired_active_rig_date)\n",
    "rig_data = rig_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8b264a4-fa3f-40d1-8616-4b324d25d0d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class RighistoricalDownloader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        rig_historical_table: str,\n",
    "        analog_well_table: list,\n",
    "        basin_of_interest: str,\n",
    "        flow_unit_of_interest: str,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the RigDownloader object with necessary parameters.\n",
    "\n",
    "        \"\"\"\n",
    "        self.rig_historical_table = rig_historical_table\n",
    "        self.analog_well_table = analog_well_table\n",
    "        self.flow_unit_of_interest = flow_unit_of_interest\n",
    "        self.basin_of_interest = basin_of_interest\n",
    "\n",
    "    def download_rig_data(self, cutoff_date, current_date) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Downloads rig data from PySpark and returns it as a Pandas DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        - spark: PySpark SparkSession.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: Rig data as Pandas DataFrame.\n",
    "        \"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT\n",
    "            date, com.api10,  com.operator, com.reservoir_gold_consolidated, \n",
    "            ana.typeCurveArea, com.BasinQuantum, ana.FlowUnit_Analog, ana.LateralLength_FT, rig_id\n",
    "        FROM\n",
    "            {self.rig_historical_table} AS com\n",
    "        INNER JOIN\n",
    "            {self.analog_well_table} AS ana\n",
    "        ON\n",
    "            ana.api10 = com.api10\n",
    "            AND ana.recentWell = 'true'\n",
    "            AND com.BasinQuantum = '{self.basin_of_interest}'\n",
    "            AND ana.FlowUnit_Analog = '{self.flow_unit_of_interest}'\n",
    "            AND date >= '{cutoff_date}'\n",
    "            AND date < '{current_date}'\n",
    "        \"\"\"\n",
    "\n",
    "        rig_data = spark.sql(query).toPandas()\n",
    "        rig_data[\"LateralLength_FT\"] = pd.to_numeric(rig_data[\"LateralLength_FT\"])\n",
    "        rig_data = rig_data.loc[rig_data.groupby(\"api10\")[\"LateralLength_FT\"].idxmax()]\n",
    "\n",
    "        return rig_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a65f3285-1a1c-4f85-8c50-67d852ba1351",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def rigs_distribution_historical_data():\n",
    "    rigtable = \"produced.private_rigs_history\"\n",
    "    analog_well_table = \"produced.analog_well_selection\"\n",
    "    rigdownload = RighistoricalDownloader(\n",
    "        rigtable, analog_well_table, basin_of_interest, flowunit_of_interest\n",
    "    )\n",
    "    rig_history_df = rigdownload.download_rig_data(training_cutoff_date, current_date)\n",
    "    return rig_history_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fb45cfb-a9b5-485c-b058-0085a2c4e035",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Get Rig Release for current rigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2e066d2-7aa7-4002-b2d2-86f45f3edb80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = spark.sql(\n",
    "    f\"select * from produced.api_level_cycle_times where scenario_id = '{scenario_id}'\"\n",
    ").toPandas()\n",
    "opr_tca_df = spark.sql(\n",
    "    f\"SELECT * FROM produced.operator_cycle_times where scenario_id = '{scenario_id}'\"\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b77fede6-68cf-4222-a9fd-05b025430ecd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_rig_release_time(opr, tca, cycle_time_df=opr_tca_df, basin_df=final_df):\n",
    "\n",
    "    if (\n",
    "        len(\n",
    "            cycle_time_df[\n",
    "                (cycle_time_df.OperatorGold == opr)\n",
    "                & (cycle_time_df.typeCurveArea == tca)\n",
    "            ]\n",
    "        )\n",
    "        > 0\n",
    "    ):\n",
    "\n",
    "        return cycle_time_df[\n",
    "            (cycle_time_df.OperatorGold == opr) & (cycle_time_df.typeCurveArea == tca)\n",
    "        ][\"time_taken_spud_to_rigrelease\"].median()\n",
    "    else:\n",
    "        if len(basin_df[basin_df.OperatorGold == opr]) > 9:\n",
    "\n",
    "            return basin_df[basin_df.OperatorGold == opr][\n",
    "                \"time_taken_spud_to_rigrelease\"\n",
    "            ].median()\n",
    "        else:\n",
    "\n",
    "            return basin_df[\"time_taken_spud_to_rigrelease\"].median()\n",
    "\n",
    "\n",
    "def get_spud_to_complete_time(opr, tca, cycle_time_df=opr_tca_df, basin_df=final_df):\n",
    "\n",
    "    if (\n",
    "        len(\n",
    "            cycle_time_df[\n",
    "                (cycle_time_df.OperatorGold == opr)\n",
    "                & (cycle_time_df.typeCurveArea == tca)\n",
    "            ]\n",
    "        )\n",
    "        > 0\n",
    "    ):\n",
    "\n",
    "        return cycle_time_df[\n",
    "            (cycle_time_df.OperatorGold == opr) & (cycle_time_df.typeCurveArea == tca)\n",
    "        ][\"time_taken_spud_to_completion\"].median()\n",
    "    else:\n",
    "        if len(basin_df[basin_df.OperatorGold == opr]) > 9:\n",
    "\n",
    "            return basin_df[basin_df.OperatorGold == opr][\n",
    "                \"time_taken_spud_to_completion\"\n",
    "            ].median()\n",
    "        else:\n",
    "\n",
    "            return basin_df[\"time_taken_spud_to_completion\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96aa8d4a-1a54-43e7-a428-aa636e6963d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rig_data[\"rig_release_date\"] = None\n",
    "rig_data[\"SpudDate\"] = pd.to_datetime(rig_data[\"SpudDate\"]).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b115fb7-2957-43c5-ba28-cdb1fa407fdb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rig_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fb75a88-4db4-4a5b-b519-7904e979dbd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n/root/.ipykernel/3846/command-1395104717220266-4074348677:10: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  if rig_release_date < pd.Timestamp(desired_active_rig_date):\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(rig_data)):\n",
    "\n",
    "    operator = rig_data[\"operator\"].iloc[i]\n",
    "    type_curve_area = rig_data[\"typeCurveArea\"].iloc[i]\n",
    "\n",
    "    spud_to_rig_release = int(get_rig_release_time(operator, type_curve_area))\n",
    "\n",
    "    rig_release_date = rig_data.at[i, \"SpudDate\"] + pd.Timedelta(\n",
    "        days=spud_to_rig_release\n",
    "    )\n",
    "\n",
    "    if rig_release_date < pd.Timestamp(desired_active_rig_date):\n",
    "        rig_release_date = pd.Timestamp(desired_active_rig_date)\n",
    "\n",
    "    rig_data.loc[i, \"rig_release_date\"] = rig_release_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96f844d2-dcc5-42e8-a9cc-380509e171a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Rigs Distribution based on User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec4e0723-d0b9-45b2-915a-31e933f24de4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_rigs_distribution_while_allocating(rigs_to_allocate, area_of_each_opr):\n",
    "    random_numbers = np.random.uniform(low=0, high=100, size=rigs_to_allocate)\n",
    "\n",
    "    def distribute_rigs(random_number):\n",
    "        for i, row in area_of_each_opr.iterrows():\n",
    "            if random_number <= row[\"cumulative_fraction_rigs\"]:\n",
    "                return row[\"operator\"]\n",
    "\n",
    "    # Apply rig distribution function to each random number\n",
    "    rigs_distribution = [\n",
    "        distribute_rigs(random_number) for random_number in random_numbers\n",
    "    ]\n",
    "\n",
    "    # Count rigs for each operator\n",
    "    rigs_count = pd.Series(rigs_distribution).value_counts()\n",
    "    rigs_count = rigs_count.reindex(area_of_each_opr[\"operator\"])\n",
    "    area_of_each_opr[\"allocated_rigs\"] = rigs_count.values\n",
    "    return area_of_each_opr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fada079-6813-4aee-aa4c-455d3809bb31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_historical_rig_distribution_among_opr(miscell_perc):\n",
    "\n",
    "    rig_history_df = rigs_distribution_historical_data()\n",
    "\n",
    "    # todo: rename the column rig_id\n",
    "    opr_rig_dist_cross_day_df = rig_history_df.groupby(\n",
    "        [\"date\", \"operator\"], as_index=False\n",
    "    )[\"rig_id\"].nunique()\n",
    "    area_of_each_opr = opr_rig_dist_cross_day_df.groupby(\"operator\", as_index=False)[\n",
    "        \"rig_id\"\n",
    "    ].sum()\n",
    "    total = area_of_each_opr[\"rig_id\"].sum()\n",
    "    area_of_each_opr[\"fraction_rigs\"] = round(\n",
    "        area_of_each_opr[\"rig_id\"] * 100 / total, 2\n",
    "    )\n",
    "\n",
    "    converted_opr_to_misc = (\n",
    "        area_of_each_opr[area_of_each_opr.fraction_rigs < miscell_perc][\"operator\"]\n",
    "        .unique()\n",
    "        .tolist()\n",
    "    )\n",
    "    area_of_each_opr.loc[\n",
    "        area_of_each_opr.fraction_rigs < miscell_perc, \"operator\"\n",
    "    ] = \"Miscellaneous\"\n",
    "    area_of_each_opr = (\n",
    "        area_of_each_opr.groupby(\"operator\", as_index=False)[\n",
    "            [\"rig_id\", \"fraction_rigs\"]\n",
    "        ]\n",
    "        .sum()\n",
    "        .sort_values(by=\"fraction_rigs\")\n",
    "    )\n",
    "    area_of_each_opr[\"cumulative_fraction_rigs\"] = area_of_each_opr[\n",
    "        \"fraction_rigs\"\n",
    "    ].cumsum()\n",
    "    return area_of_each_opr, converted_opr_to_misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33c7714f-12e1-4ebe-9825-a13a5cabcfc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def allocate_rigs(area_of_each_opr, today_rigs_df, date):\n",
    "    date_of_allocation = datetime.datetime.strptime(date, \"%Y-%m-%d\").date()\n",
    "    rig_count = 1\n",
    "    for i, row in area_of_each_opr.iterrows():\n",
    "        if row[\"allocated_rigs\"] > 0:\n",
    "            total_rigs = int(row[\"allocated_rigs\"])\n",
    "            curr_opr = row[\"operator\"]\n",
    "            for j in range(total_rigs):\n",
    "                rig_dict = {\n",
    "                    \"rig_id\": f\"allocated_rig_{rig_count}\",\n",
    "                    \"api10\": None,\n",
    "                    \"operator\": curr_opr,\n",
    "                    \"typeCurveArea\": None,\n",
    "                    \"SpudDate\": None,\n",
    "                    \"rig_release_date\": date_of_allocation,\n",
    "                }\n",
    "                rig_count += 1\n",
    "                today_rigs_df = today_rigs_df.append(rig_dict, ignore_index=True)\n",
    "    return today_rigs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c22f239c-f7f0-427a-b577-421ee7b16dfa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def get_rig_distribution_while_deallocating(area_of_each_opr1, today_rigs_df, total_rigs):\n",
    "\n",
    "#     contains_negative = True\n",
    "#     def distribute_rigs(random_number):\n",
    "#         for i, row in area_of_each_opr1.iterrows():\n",
    "#             if random_number <= row['cumulative_fraction_rigs']:\n",
    "#                 return row['operator']\n",
    "#     while contains_negative==True:\n",
    "#         random_numbers = np.random.uniform(low=0, high=100, size=total_rigs)\n",
    "\n",
    "#         # Apply rig distribution function to each random number\n",
    "#         rigs_distribution = [distribute_rigs(random_number) for random_number in random_numbers]\n",
    "\n",
    "#         # Count rigs for each operator\n",
    "#         rigs_count = pd.Series(rigs_distribution).value_counts()\n",
    "#         rigs_count = rigs_count.reindex(area_of_each_opr1['operator'])\n",
    "#         area_of_each_opr1['allocate_rigs'] = rigs_count.values\n",
    "#         area_of_each_opr1 = area_of_each_opr1[area_of_each_opr1.allocate_rigs>0]\n",
    "#         rigs_on_each_opr = today_rigs_df.groupby(\"operator\", as_index=False)['rig_id'].nunique()\n",
    "#         new_df = pd.merge(rigs_on_each_opr, area_of_each_opr1[['operator', 'allocate_rigs']], on='operator', how='outer')\n",
    "#         new_df[\"rigs_to_remove\"] = new_df['rig_id']-new_df['allocate_rigs']\n",
    "#         contains_negative = any(new_df['rigs_to_remove'] < 0)\n",
    "#     return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5add97ce-e416-4ba3-89fe-2b8e735329da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def deallocate_and_reallocate_rigs(required_rigs_distribution_df, today_rigs_df, date):\n",
    "    date_of_allocation = datetime.datetime.strptime(date, \"%Y-%m-%d\").date()\n",
    "\n",
    "    required_rigs_distribution_df = required_rigs_distribution_df[\n",
    "        required_rigs_distribution_df.allocated_rigs.notnull()\n",
    "    ][[\"operator\", \"allocated_rigs\"]]\n",
    "\n",
    "    total_current_rigs = (\n",
    "        today_rigs_df.groupby(\"operator\", as_index=False)[\"rig_id\"]\n",
    "        .nunique()\n",
    "        .rename(columns={\"rig_id\": \"current_num_of_rigs\"})\n",
    "    )\n",
    "\n",
    "    required_rigs_distribution_df = pd.merge(\n",
    "        required_rigs_distribution_df, total_current_rigs, on=\"operator\", how=\"outer\"\n",
    "    )\n",
    "\n",
    "    required_rigs_distribution_df.loc[\n",
    "        required_rigs_distribution_df.current_num_of_rigs.isnull(),\n",
    "        \"current_num_of_rigs\",\n",
    "    ] = 0\n",
    "\n",
    "    required_rigs_distribution_df = required_rigs_distribution_df[\n",
    "        required_rigs_distribution_df.allocated_rigs.notnull()\n",
    "    ]\n",
    "\n",
    "    required_rigs_distribution_df[\"rigs_to_remove\"] = (\n",
    "        required_rigs_distribution_df[\"current_num_of_rigs\"]\n",
    "        - required_rigs_distribution_df[\"allocated_rigs\"]\n",
    "    )\n",
    "    rig_count = 1\n",
    "    for i, row in required_rigs_distribution_df.iterrows():\n",
    "        opr = row[\"operator\"]\n",
    "        rigs_to_remove = row[\"rigs_to_remove\"]\n",
    "        allocate_rigs = row[\"allocated_rigs\"]\n",
    "\n",
    "        if rigs_to_remove > 0:\n",
    "            rigs_api_to_remove = (\n",
    "                today_rigs_df[today_rigs_df.operator == opr]\n",
    "                .sort_values(by=\"rig_release_date\")\n",
    "                .reset_index(drop=True)\n",
    "                .head(int(rigs_to_remove))[\"rig_id\"]\n",
    "                .tolist()\n",
    "            )\n",
    "            today_rigs_df = today_rigs_df[\n",
    "                ~today_rigs_df.rig_id.isin(rigs_api_to_remove)\n",
    "            ]\n",
    "        elif rigs_to_remove < 0:\n",
    "\n",
    "            for j in range(int(abs(rigs_to_remove))):\n",
    "                rig_dict = {\n",
    "                    \"rig_id\": f\"allocated_rig_{rig_count}\",\n",
    "                    \"api10\": None,\n",
    "                    \"operator\": opr,\n",
    "                    \"typeCurveArea\": None,\n",
    "                    \"SpudDate\": None,\n",
    "                    \"rig_release_date\": date_of_allocation,\n",
    "                }\n",
    "                rig_count += 1\n",
    "                today_rigs_df = today_rigs_df.append(rig_dict, ignore_index=True)\n",
    "\n",
    "    return today_rigs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "665c19e6-ad7c-481a-aaed-a5aa7d9d54d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/pandas/utils.py:109: UserWarning: The conversion of DecimalType columns is inefficient and may take a long time. Column names: [LateralLength_FT] If those columns are not necessary, you may consider dropping them or converting to primitive types before the conversion.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\nhere2\nhere3\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.ipykernel/3846/command-1395104717220272-2125759617:40: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  today_rigs_df = today_rigs_df.append(rig_dict, ignore_index=True)\n/root/.ipykernel/3846/command-1395104717220272-2125759617:40: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  today_rigs_df = today_rigs_df.append(rig_dict, ignore_index=True)\n/root/.ipykernel/3846/command-1395104717220272-2125759617:40: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  today_rigs_df = today_rigs_df.append(rig_dict, ignore_index=True)\n/root/.ipykernel/3846/command-1395104717220272-2125759617:40: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  today_rigs_df = today_rigs_df.append(rig_dict, ignore_index=True)\n/root/.ipykernel/3846/command-1395104717220272-2125759617:26: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n  rigs_api_to_remove = today_rigs_df[today_rigs_df.operator == opr].sort_values(by='rig_release_date').reset_index(drop=True).head(int(rigs_to_remove))['rig_id'].tolist()\n/root/.ipykernel/3846/command-1395104717220272-2125759617:40: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  today_rigs_df = today_rigs_df.append(rig_dict, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "current_rigs = len(rig_data)\n",
    "area_of_each_opr, miss_oprs = get_historical_rig_distribution_among_opr(\n",
    "    cutoff_misc_perc\n",
    ")\n",
    "miscellaneous_opr = pd.DataFrame({\"real_operator_name\": miss_oprs})\n",
    "miscellaneous_opr[\"new_opr_name\"] = \"Miscellaneous\"\n",
    "\n",
    "if len(miscellaneous_opr) == 0:\n",
    "    miscellaneous_opr = pd.DataFrame({\"real_operator_name\": [\"dummy_operator\"]})\n",
    "    miscellaneous_opr[\"new_opr_name\"] = None\n",
    "\n",
    "# modifing rig data for miscellaneous category\n",
    "rig_data.loc[\n",
    "    rig_data.operator.isin(miscellaneous_opr.real_operator_name.unique()), \"operator\"\n",
    "] = \"Miscellaneous\"\n",
    "\n",
    "if user_input_num_of_rigs != \"null\":\n",
    "\n",
    "    user_input_num_of_rigs = int(user_input_num_of_rigs)\n",
    "\n",
    "    if user_input_num_of_rigs > current_rigs:\n",
    "\n",
    "        rigs_to_allocate = int(user_input_num_of_rigs - current_rigs)\n",
    "        area_of_each_opr = get_rigs_distribution_while_allocating(\n",
    "            rigs_to_allocate, area_of_each_opr\n",
    "        )\n",
    "\n",
    "        rig_data = allocate_rigs(area_of_each_opr, rig_data, desired_active_rig_date)\n",
    "\n",
    "    elif user_input_num_of_rigs < current_rigs:\n",
    "        rigs_to_deallocate_df = get_rigs_distribution_while_allocating(\n",
    "            user_input_num_of_rigs, area_of_each_opr\n",
    "        )\n",
    "        rig_data = deallocate_and_reallocate_rigs(\n",
    "            rigs_to_deallocate_df, rig_data, desired_active_rig_date\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64203545-802c-4550-b76f-c6e6e15fa191",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rig_data.rename({\"SpudDate\": \"spud_date\"}, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63795c4f-ebe3-4be5-8a73-b05b80bc13e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rig_data = rig_data[\n",
    "    [\n",
    "        \"date\",\n",
    "        \"rig_id\",\n",
    "        \"api10\",\n",
    "        \"reservoir_gold_consolidated\",\n",
    "        \"BasinQuantum\",\n",
    "        \"operator\",\n",
    "        \"typeCurveArea\",\n",
    "        \"FlowUnit_Analog\",\n",
    "        \"spud_date\",\n",
    "        \"rig_release_date\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72b7f88b-2655-49ed-bd08-df9f4d3be05b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rig_data[\"rig_release_date\"] = pd.to_datetime(rig_data[\"rig_release_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0ac83a6-4ed1-4c74-9e97-de094149fa1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rig_data[\"BasinQuantum\"].fillna(basin_of_interest, inplace=True)\n",
    "rig_data[\"FlowUnit_Analog\"].fillna(flowunit_of_interest, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21e8e77f-c76b-4b57-8247-5ad2558bc773",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rig_data[\"scenario_id\"] = scenario_id\n",
    "miscellaneous_opr[\"scenario_id\"] = scenario_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2c2a2ff-ae3f-4f25-9ed8-60ebb48756c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Creating Tabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2622b5f6-9f48-4cbe-8640-664a276c0388",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "          delete from produced.rig_model_miscellaneous_opr where scenario_id = \"{scenario_id}\"\n",
    "          \"\"\"\n",
    ")\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "          delete from produced.rig_model_table where scenario_id = \"{scenario_id}\"\n",
    "          \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "727ec5c5-241c-4d9f-b235-559e62acd4d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(miscellaneous_opr).write.format(\"delta\").option(\n",
    "    \"mergeSchema\", \"true\"\n",
    ").mode(\"append\").saveAsTable(f\"produced.rig_model_miscellaneous_opr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f7fe1f8-dcac-449d-ae1c-52e51621b7dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rig_data[\"date\"] = pd.to_datetime(rig_data[\"date\"])\n",
    "rig_data[\"spud_date\"] = pd.to_datetime(rig_data[\"spud_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dfece33-2d7f-4179-bd31-703e8a56f7da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rig_data[\"date\"] = rig_data[\"date\"].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1de5982f-a53f-4319-a484-dfcfae0b00ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(rig_data).write.format(\"delta\").option(\n",
    "    \"mergeSchema\", \"true\"\n",
    ").mode(\"append\").saveAsTable(f\"produced.rig_model_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20e42b83-ee63-46fd-8961-ec189ec125da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02-RIG-MODEL",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
