{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9161170-6755-4c54-ae5b-cb090bfd177b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46797d03-7147-4090-b7db-5cf87b0fa5d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import DateType, FloatType, IntegerType\n",
    "from pyspark.sql.functions import desc, col\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abb2c421-a479-494e-ba00-755b999a8773",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88683cff-6368-461a-b35e-8e878bc489ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# basin_of_interest = \"GULF COAST EAST\"\n",
    "# flowunit_of_interest = \"HAYNESVILLE\"\n",
    "# desired_minimum_date_for_producing_wells = \"2022-04-01\"\n",
    "# desired_maximum_date_for_producing_wells = \"2034-04-01\"\n",
    "# wip_well_date = \"2023-10-01\"\n",
    "# buffer_days_for_rig_movement = 10\n",
    "# cutoff_first_prod_date_for_wip_wells = \"2023-10-01\"\n",
    "# cutoff_first_prod_date_for_wip_wells = datetime.datetime.strptime(cutoff_first_prod_date_for_wip_wells, \"%Y-%m-%d\").date()\n",
    "# desired_first_prod_date_for_wip_wells =\"2023-10-01\"\n",
    "# desired_first_prod_date_for_wip_wells = datetime.datetime.strptime(desired_first_prod_date_for_wip_wells, \"%Y-%m-%d\").date()\n",
    "# cutoff_date_for_new_wells = \"2023-10-01\"\n",
    "# current_date = '2024-04-29'\n",
    "# desired_minimum_firstproddate_for_completed_drilled_wells = datetime.datetime.strptime(\"2020-01-01\", \"%Y-%m-%d\").date()\n",
    "# scenario_id = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62c7034e-498d-480d-9891-1eecd0263adc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "basin_of_interest = dbutils.widgets.get(\"basin_of_interest\")\n",
    "flowunit_of_interest = dbutils.widgets.get(\"flowunit_of_interest\")\n",
    "desired_minimum_date_for_producing_wells = dbutils.widgets.get(\n",
    "    \"desired_minimum_date_for_producing_wells\"\n",
    ")\n",
    "\n",
    "desired_maximum_date_for_producing_wells = dbutils.widgets.get(\n",
    "    \"desired_maximum_date_for_producing_wells\"\n",
    ")\n",
    "\n",
    "wip_well_date = dbutils.widgets.get(\"desired_wip_wells_date\")\n",
    "\n",
    "buffer_days_for_rig_movement = int(dbutils.widgets.get(\"buffer_days_for_rig_movement\"))\n",
    "\n",
    "cutoff_first_prod_date_for_wip_wells = dbutils.widgets.get(\n",
    "    \"cutoff_first_prod_date_for_wip_wells\"\n",
    ")\n",
    "\n",
    "cutoff_first_prod_date_for_wip_wells = datetime.datetime.strptime(\n",
    "    cutoff_first_prod_date_for_wip_wells, \"%Y-%m-%d\"\n",
    ").date()\n",
    "\n",
    "desired_first_prod_date_for_wip_wells = dbutils.widgets.get(\n",
    "    \"desired_first_prod_date_for_wip_wells\"\n",
    ")\n",
    "\n",
    "desired_first_prod_date_for_wip_wells = datetime.datetime.strptime(\n",
    "    desired_first_prod_date_for_wip_wells, \"%Y-%m-%d\"\n",
    ").date()\n",
    "\n",
    "cutoff_date_for_new_wells = dbutils.widgets.get(\"cutoff_date_for_new_wells\")\n",
    "current_date = dbutils.widgets.get(\"current_date\")\n",
    "\n",
    "desired_minimum_firstproddate_for_completed_drilled_wells = dbutils.widgets.get(\n",
    "    \"desired_minimum_firstproddate_for_completed_drilled_wells\"\n",
    ")\n",
    "\n",
    "desired_minimum_firstproddate_for_completed_drilled_wells = datetime.datetime.strptime(\n",
    "    desired_minimum_firstproddate_for_completed_drilled_wells, \"%Y-%m-%d\"\n",
    ").date()\n",
    "scenario_id = dbutils.widgets.get(\"scenario_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97172e11-a58b-4ff8-b8df-7e98dcb8f281",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# TypeCurve dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5394f037-4af2-43a7-9d60-fc223cffaeba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "typecurve_df_actual = spark.sql(\n",
    "    f\"\"\"\n",
    "  SELECT\n",
    "  *\n",
    "EXCEPT(LateralLength_FT, fu_median_ll, tca_median_ll),\n",
    "  COALESCE(LateralLength_FT, tca_median_ll, fu_median_ll) as LateralLength_FT\n",
    "FROM(\n",
    "    SELECT\n",
    "      API10,\n",
    "      API14,\n",
    "      typeCurveArea,\n",
    "      FlowUnit_Analog,\n",
    "      ReservoirGoldConsolidated,\n",
    "      basinQuantum,\n",
    "      LateralLength_FT,\n",
    "      spudDate,\n",
    "      FirstProdDate,\n",
    "      OperatorGold,\n",
    "      WellStatus,\n",
    "      HoleDirection,\n",
    "      PERCENTILE_CONT(0.5) WITHIN GROUP (\n",
    "        ORDER BY\n",
    "          LateralLength_FT\n",
    "      ) OVER () AS fu_median_ll,\n",
    "      PERCENTILE_CONT(0.5) WITHIN GROUP (\n",
    "        ORDER BY\n",
    "          LateralLength_FT\n",
    "      ) OVER(PARTITION BY typeCurveArea) AS tca_median_ll\n",
    "    FROM\n",
    "      produced.analog_well_selection\n",
    "    WHERE\n",
    "      recentWell = \"true\"\n",
    "      AND flowUnit_Analog = '{flowunit_of_interest}'\n",
    "  )\n",
    "\"\"\"\n",
    ")\n",
    "typecurve_df_actual.createOrReplaceTempView(\"analog_wells\")\n",
    "typecurve_df = typecurve_df_actual.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4461ee1e-007f-41f0-94fb-d5fac816fd95",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# WIP wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2c81d39-7b0d-4ce4-9799-d37473ef4e03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class WIPDownloader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        rig_data_table: str,\n",
    "        basin_of_interest: str,\n",
    "        analog_wells_view,\n",
    "        flowunit_of_interest: str,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the RigDownloader object with necessary parameters.\n",
    "\n",
    "        Parameters:\n",
    "        - rig_data_table (str): Table name for rig data.\n",
    "        - rig_data_columns (list): Columns to select for rig data.\n",
    "        - basin_of_interest (str): Basin of interest.\n",
    "        \"\"\"\n",
    "        self.rig_data_table1 = rig_data_table\n",
    "        self.basin_of_interest = basin_of_interest\n",
    "        self.analog_wells_view = analog_wells_view\n",
    "        self.flowunit_of_interest = flowunit_of_interest\n",
    "\n",
    "    def download_wip_data(self, spark, typecurve_df, cutoff_date) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Downloads rig data from PySpark and returns it as a Pandas DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        - spark: PySpark SparkSession.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: Rig data as Pandas DataFrame.\n",
    "        \"\"\"\n",
    "\n",
    "        rig_data = spark.sql(\n",
    "            f\"\"\"\n",
    "                  SELECT rigs.date, \n",
    "                    rigs.API10, \n",
    "                    rigs.operator as OperatorGold, \n",
    "                    rigs.BasinQuantum, \n",
    "                    ana.LateralLength_FT, \n",
    "                    ana.spudDate as wip_spud_date,\n",
    "                    ana.typeCurveArea,\n",
    "                    ana.FlowUnit_Analog,\n",
    "                    ana.ReservoirGoldConsolidated\n",
    "                  FROM {self.rig_data_table1} rigs\n",
    "                  INNER JOIN {self.analog_wells_view} ana\n",
    "                  on ana.API10 = rigs.API10\n",
    "                  WHERE\n",
    "                  rigs.BasinQuantum = '{self.basin_of_interest}'\n",
    "                  AND ana.flowUnit_Analog = '{self.flowunit_of_interest}'\n",
    "                  AND rigs.date = '{cutoff_date}'\n",
    "                  \"\"\"\n",
    "        ).toPandas()\n",
    "\n",
    "        rig_data = rig_data.groupby(\"API10\", as_index=False).first()\n",
    "        return rig_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acd0902c-c911-4c42-96e7-2c8342382d5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rigtable = \"produced.private_rigs_history\"\n",
    "well_completion_table = \"analog_wells\"\n",
    "wipdownload = WIPDownloader(\n",
    "    rigtable, basin_of_interest, well_completion_table, flowunit_of_interest\n",
    ")\n",
    "wip_well_data = wipdownload.download_wip_data(spark, typecurve_df, wip_well_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bd90042-d1bb-4672-8e53-4249e1c35f94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Adding Drilling Wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6987e79e-6411-4178-86e1-bac33c517016",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drilling_wells = typecurve_df[typecurve_df.WellStatus == \"DRILLING\"]\n",
    "drilling_wells = drilling_wells[\n",
    "    ~drilling_wells.API10.isin(wip_well_data.API10.unique())\n",
    "]\n",
    "\n",
    "drilling_wells[\"date\"] = None\n",
    "drilling_wells[\"BasinQuantum\"] = basin_of_interest\n",
    "drilling_wells.rename({\"spudDate\": \"wip_spud_date\"}, inplace=True, axis=1)\n",
    "drilling_wells = drilling_wells[wip_well_data.columns]\n",
    "\n",
    "wip_well_data = pd.concat([wip_well_data, drilling_wells])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "653ebb41-4c22-4b2b-be0f-b5515aa2d36b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "opr_tca_df = spark.sql(\n",
    "    f\"SELECT * FROM produced.operator_cycle_times where scenario_id = '{scenario_id}'  \"\n",
    ").toPandas()\n",
    "final_df = spark.sql(\n",
    "    f\"select * from produced.api_level_cycle_times where scenario_id = '{scenario_id}' \"\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac7aabc2-ed26-4ece-9819-78487261518d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# merge cycle time information on operator typecurve level to wip data\n",
    "wip_well_data = pd.merge(\n",
    "    wip_well_data, opr_tca_df, on=[\"OperatorGold\", \"typeCurveArea\"], how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c551391-75d8-4886-8c68-fddf7fae3dad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wip_well_data[\"time_taken_spud_to_rigrelease\"] = wip_well_data[\n",
    "    \"time_taken_spud_to_rigrelease\"\n",
    "].fillna(final_df[\"time_taken_spud_to_rigrelease\"].median())\n",
    "\n",
    "wip_well_data[\"time_taken_spud_to_completion\"] = wip_well_data[\n",
    "    \"time_taken_spud_to_completion\"\n",
    "].fillna(final_df[\"time_taken_spud_to_completion\"].median())\n",
    "\n",
    "wip_well_data[\"time_taken_completion_to_firstprod\"] = wip_well_data[\n",
    "    \"time_taken_completion_to_firstprod\"\n",
    "].fillna(final_df[\"time_taken_completion_to_firstprod\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ffd2d75-497e-4fdd-b2e2-14332a257949",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Will Try to use Pandas for below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "406de969-3d40-4776-9497-7f7b613ef69f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def drilling_and_scheduler_wip(\n",
    "    wip_df,\n",
    "    buffer_days_for_rig_movement,\n",
    "    cutoff_first_prod_date_for_wip_wells,\n",
    "    desired_first_prod_date_for_wip_wells,\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs scheduling for WIP wells, calculating rig release, completion, and first production dates.\n",
    "\n",
    "    Parameters:\n",
    "    - wip_df (pd.DataFrame): DataFrame containing WIP well data.\n",
    "    - buffer_days (int): Buffer days to be added.\n",
    "    - user_input_for_present_date (format: YYYY-MM-DD)): makeing First prod date to present date\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with updated scheduling information.\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(len(wip_df)):\n",
    "        if wip_df[\"wip_spud_date\"].iloc[i] is not None:\n",
    "\n",
    "            wip_df.at[i, \"wip_rig_release_date\"] = wip_df.at[\n",
    "                i, \"wip_spud_date\"\n",
    "            ] + pd.to_timedelta(wip_df.at[i, \"time_taken_spud_to_rigrelease\"])\n",
    "\n",
    "            wip_df.at[i, \"wip_completion_date\"] = wip_df.at[\n",
    "                i, \"wip_spud_date\"\n",
    "            ] + pd.to_timedelta(wip_df.at[i, \"time_taken_spud_to_completion\"])\n",
    "\n",
    "            first_prod_date = wip_df.at[i, \"wip_spud_date\"] + pd.to_timedelta(\n",
    "                wip_df.at[i, \"time_taken_spud_to_completion\"]\n",
    "            )\n",
    "\n",
    "            # if first prod date comes in the past, we will make it today\n",
    "            if first_prod_date < cutoff_first_prod_date_for_wip_wells:\n",
    "                first_prod_date = desired_first_prod_date_for_wip_wells\n",
    "\n",
    "            wip_df.loc[i, \"wip_firstprod_date\"] = first_prod_date\n",
    "\n",
    "    return wip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f58e8c4d-9b82-4bae-b116-282c3576ac5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wip_well_data[\"wip_rig_release_date\"] = None\n",
    "wip_well_data[\"wip_completion_date\"] = None\n",
    "wip_well_data[\"wip_firstprod_date\"] = None\n",
    "\n",
    "wip_well_data = drilling_and_scheduler_wip(\n",
    "    wip_well_data,\n",
    "    buffer_days_for_rig_movement,\n",
    "    cutoff_first_prod_date_for_wip_wells,\n",
    "    desired_first_prod_date_for_wip_wells,\n",
    ")\n",
    "\n",
    "# removing all wip wells for which we don't have type curve information\n",
    "wip_well_data = wip_well_data.dropna(subset=[\"typeCurveArea\"], axis=0)\n",
    "\n",
    "# converting first prod dates to first day of the month\n",
    "wip_well_data[\"wip_firstprod_date_1\"] = (\n",
    "    pd.to_datetime(wip_well_data[\"wip_firstprod_date\"])\n",
    "    .dt.to_period(\"M\")\n",
    "    .dt.to_timestamp()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f49818c-7c39-407c-843b-1c4aec23f233",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Type Curve Production estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9db465e-0e29-42ec-93cb-c25611507b5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class TCAProductionDownloader:\n",
    "    \"\"\"\n",
    "    Class for downloading TCA production data using PySpark.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, tca_production_table: str, basin_of_interest: str, tca_production_columns\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the TCAProductionDownloader.\n",
    "\n",
    "        Parameters:\n",
    "        - tca_production_table (str): Name of the TCA production table.\n",
    "        - basin_of_interest (str): Basin of interest for filtering data.\n",
    "        \"\"\"\n",
    "        self.tca_production_table_name = tca_production_table\n",
    "        self.basin_of_interest = basin_of_interest\n",
    "        self.tca_production_columns = tca_production_columns\n",
    "\n",
    "    def get_tca_production_data(self, spark) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get TCA production data from the specified table.\n",
    "\n",
    "        Parameters:\n",
    "        - spark: PySpark session.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: Produced DataFrame.\n",
    "        \"\"\"\n",
    "        typecurve_qcast_forecast = (\n",
    "            spark.table(self.tca_production_table_name)\n",
    "            .select(*self.tca_production_columns)\n",
    "            .filter(col(\"BasinQuantum\") == self.basin_of_interest)\n",
    "            .toPandas()\n",
    "        )\n",
    "        return typecurve_qcast_forecast\n",
    "\n",
    "    def download_final_data(self, spark):\n",
    "        \"\"\"\n",
    "        Download final TCA production data.\n",
    "\n",
    "        Parameters:\n",
    "        - spark: PySpark session.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: Produced DataFrame.\n",
    "        \"\"\"\n",
    "        final_df = self.get_tca_production_data(spark)\n",
    "\n",
    "        # converting gas unit from MCF to BCF\n",
    "        # converting water and oil unit from BBL to MBO\n",
    "        final_df[\"gas_combined_monthly\"] = final_df[\"gas_combined_monthly\"] / 10**6\n",
    "        final_df[\"oil_combined_monthly\"] = final_df[\"oil_combined_monthly\"] / 10**3\n",
    "        final_df[\"water_combined_monthly\"] = (\n",
    "            final_df[\"water_combined_monthly\"] / 10**3\n",
    "        )\n",
    "\n",
    "        final_df.rename(\n",
    "            {\n",
    "                \"gas_combined_monthly\": \"Gas_BCF\",\n",
    "                \"oil_combined_monthly\": \"Oil_MBO\",\n",
    "                \"water_combined_monthly\": \"Water_MBO\",\n",
    "                \"index\": \"producing_month\",\n",
    "            },\n",
    "            axis=1,\n",
    "            inplace=True,\n",
    "        )\n",
    "        return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93ae9d6f-3392-4690-9d0c-323338de2cda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/pandas/utils.py:124: UserWarning:\n\nThe conversion of DecimalType columns is inefficient and may take a long time. Column names: [oil_combined_monthly, gas_combined_monthly, water_combined_monthly, normalization_value] If those columns are not necessary, you may consider dropping them or converting to primitive types before the conversion.\n\n"
     ]
    }
   ],
   "source": [
    "tcatable = \"produced.typecurve_qcast_forecast\"\n",
    "tca_production_columns = (\n",
    "    \"api_list\",\n",
    "    \"BasinQuantum\",\n",
    "    \"typeCurveArea\",\n",
    "    \"date\",\n",
    "    \"index\",\n",
    "    \"oil_combined_monthly\",\n",
    "    \"gas_combined_monthly\",\n",
    "    \"water_combined_monthly\",\n",
    "    \"normalization_value\",\n",
    ")\n",
    "tcadownload = TCAProductionDownloader(\n",
    "    tcatable, basin_of_interest, tca_production_columns\n",
    ")\n",
    "tca_produnction_df = tcadownload.download_final_data(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a46160ab-5619-490d-8f96-cb69226ebdd4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Inventories Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64e329f7-fe1f-43a3-8ba2-ba2ae6cd7c9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "inventory_rigs_df = spark.sql(\n",
    "    f\"select * from produced.inventory_drilling_scheduling_table where Status = 'Drilled' and scenario_id = '{scenario_id}' \"\n",
    ").toPandas()\n",
    "\n",
    "# converting inventory_firstprod_date to first day of month\n",
    "inventory_rigs_df[\"inventory_firstprod_date\"] = (\n",
    "    pd.to_datetime(inventory_rigs_df[\"inventory_firstprod_date\"])\n",
    "    .dt.to_period(\"M\")\n",
    "    .dt.to_timestamp()\n",
    ")\n",
    "\n",
    "# merging inventory data with tca prod data\n",
    "inventory_production = pd.merge(\n",
    "    inventory_rigs_df,\n",
    "    tca_produnction_df[\n",
    "        [\"typeCurveArea\", \"producing_month\", \"Oil_MBO\", \"Gas_BCF\", \"Water_MBO\"]\n",
    "    ],\n",
    "    on=\"typeCurveArea\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "inventory_production = inventory_production[\n",
    "    (inventory_production.producing_month.notnull())\n",
    "    & (inventory_production.inventory_firstprod_date.notnull())\n",
    "]\n",
    "\n",
    "# calculate production date based on inventory first production date and producing month\n",
    "inventory_production[\"production_date\"] = (\n",
    "    (inventory_production[\"inventory_firstprod_date\"].dt.to_period(\"M\"))\n",
    "    + inventory_production[\"producing_month\"]\n",
    "    - 1\n",
    ").dt.to_timestamp()\n",
    "\n",
    "inventory_production.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d490a62-c2c7-405e-b3a1-3656e1c98526",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# WIP well production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aca2f0fc-fc15-412f-a0ac-32be0d8027bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# merging wip data and tca production data to have\n",
    "wip_well_production = pd.merge(\n",
    "    wip_well_data,\n",
    "    tca_produnction_df[\n",
    "        [\"typeCurveArea\", \"producing_month\", \"Oil_MBO\", \"Gas_BCF\", \"Water_MBO\"]\n",
    "    ],\n",
    "    on=\"typeCurveArea\",\n",
    "    how=\"left\",\n",
    "    validate=\"many_to_many\",\n",
    ")\n",
    "\n",
    "wip_well_production = wip_well_production[wip_well_production.producing_month.notnull()]\n",
    "\n",
    "# getting production month for next 50 years based on the first prod date for each wip well\n",
    "wip_well_production[\"production_date\"] = (\n",
    "    (pd.to_datetime(wip_well_production[\"wip_firstprod_date\"]).dt.to_period(\"M\"))\n",
    "    + wip_well_production[\"producing_month\"].astype(int)\n",
    "    - 1\n",
    ").dt.to_timestamp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c1121ce-2bdc-40c0-b4c0-33932c6eae11",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Producing Wells 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e7e91ac-262f-477d-a55a-beb344a6f87a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ProducingWellDownloader:\n",
    "    \"\"\"\n",
    "    Class for downloading producing well data using PySpark.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        qcast_forecast_table: str,\n",
    "        qcast_forecast_table_columns,\n",
    "        basin_of_interest: str,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the ProducingWellDownloader.\n",
    "\n",
    "        Parameters:\n",
    "        - qcast_forecast_table (str): Name of the QCAST forecast table.\n",
    "        - tca_data_table (str): Name of the TCS data table.\n",
    "        - prod_com_table (str): Name of the production completion table.\n",
    "        - basin_of_interest (str): Basin of interest for filtering data.\n",
    "        \"\"\"\n",
    "        self.qcast_forecast_table = qcast_forecast_table\n",
    "        self.qcast_forecast_table_columns = qcast_forecast_table_columns\n",
    "        self.basin_of_interest = basin_of_interest\n",
    "\n",
    "    def download_producing_data(self, spark, start_date, end_date) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Download producing data from QCAST forecast table.\n",
    "\n",
    "        Parameters:\n",
    "        - spark: PySpark session.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: Produced DataFrame.\n",
    "        \"\"\"\n",
    "        qcast_forecast = (\n",
    "            spark.table(self.qcast_forecast_table)\n",
    "            .select(*self.qcast_forecast_table_columns)\n",
    "            .filter(\n",
    "                (col(\"BasinQuantum\") == self.basin_of_interest)\n",
    "                & (\n",
    "                    col(\"date\").cast(DateType())\n",
    "                    < datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "                )\n",
    "                & (\n",
    "                    col(\"date\").cast(DateType())\n",
    "                    > datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "                )\n",
    "            )\n",
    "            .toPandas()\n",
    "        )\n",
    "        return qcast_forecast\n",
    "\n",
    "    def download_final_data(\n",
    "        self, spark, typecurve_df, cutoff_min_date, cutoff_max_date, merge_how=\"left\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Retrieve the ultimate dataset by combining information from production, TCS, and production completion data. In this process, we extract operator and lateral length values for production from TCS data and subsequently populate any missing values from SPR data.\n",
    "\n",
    "        Parameters:\n",
    "        - spark: PySpark session.\n",
    "        - typecurve_df: typecurve infomration for api10.\n",
    "        - merge_how (str): Type of merge to be performed (default: 'left').\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: Produced DataFrame.\n",
    "        \"\"\"\n",
    "\n",
    "        df = self.download_producing_data(spark, cutoff_min_date, cutoff_max_date)\n",
    "        df.rename({\"api\": \"API10\"}, inplace=True, axis=1)\n",
    "\n",
    "        df = pd.merge(\n",
    "            df,\n",
    "            typecurve_df[\n",
    "                [\n",
    "                    \"API10\",\n",
    "                    \"typeCurveArea\",\n",
    "                    \"FlowUnit_Analog\",\n",
    "                    \"LateralLength_FT\",\n",
    "                    \"ReservoirGoldConsolidated\",\n",
    "                    \"OperatorGold\",\n",
    "                ]\n",
    "            ],\n",
    "            on=\"API10\",\n",
    "            how=\"inner\",\n",
    "        )\n",
    "\n",
    "        df[\"gas_combined_monthly\"] = df[\"gas_combined_monthly\"] / 10**6\n",
    "        df[\"oil_combined_monthly\"] = df[\"oil_combined_monthly\"] / 10**3\n",
    "        df[\"water_combined_monthly\"] = df[\"water_combined_monthly\"] / 10**3\n",
    "\n",
    "        df.rename(\n",
    "            {\n",
    "                \"gas_combined_monthly\": \"Gas_BCF\",\n",
    "                \"oil_combined_monthly\": \"Oil_MBO\",\n",
    "                \"water_combined_monthly\": \"Water_MBO\",\n",
    "                \"index\": \"producing_month\",\n",
    "            },\n",
    "            axis=1,\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1be30e7-f2a9-4c71-a980-db6d7e152b01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/pandas/utils.py:124: UserWarning:\n\nThe conversion of DecimalType columns is inefficient and may take a long time. Column names: [oil_combined_monthly, gas_combined_monthly, water_combined_monthly] If those columns are not necessary, you may consider dropping them or converting to primitive types before the conversion.\n\n"
     ]
    }
   ],
   "source": [
    "pdptable = \"produced.autodca_qcast_forecast\"\n",
    "pdptable_col = (\n",
    "    \"api\",\n",
    "    \"date\",\n",
    "    \"index\",\n",
    "    \"oil_combined_monthly\",\n",
    "    \"gas_combined_monthly\",\n",
    "    \"water_combined_monthly\",\n",
    ")\n",
    "pdpdownload = ProducingWellDownloader(pdptable, pdptable_col, basin_of_interest)\n",
    "pdp_df = pdpdownload.download_final_data(\n",
    "    spark,\n",
    "    typecurve_df,\n",
    "    desired_minimum_date_for_producing_wells,\n",
    "    desired_maximum_date_for_producing_wells,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "963e0f3c-a101-49cb-9a95-45597464c27b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# removing future api\n",
    "current_date = datetime.datetime.strptime(current_date, \"%Y-%m-%d\").date()\n",
    "\n",
    "api_to_remove = pdp_df[(pdp_df.producing_month == 1) & (pdp_df.date >= current_date)][\n",
    "    \"API10\"\n",
    "].unique()\n",
    "pdp_df = pdp_df[~pdp_df.API10.isin(api_to_remove)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8ff2397-e550-4135-96c0-730310eece27",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Producing Wells 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98d2bf6e-4334-4cd6-9274-56010c0d0153",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ProducingWellDownloader2:\n",
    "    \"\"\"\n",
    "    Class for downloading producing well data using PySpark.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        merge_production_table: str,\n",
    "        merge_production_table_columns,\n",
    "        pdp_df1_api_list,\n",
    "        analog_well_table,\n",
    "        basin_of_interest: str,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the ProducingWellDownloader.\n",
    "\n",
    "        Parameters:\n",
    "        - qcast_forecast_table (str): Name of the QCAST forecast table.\n",
    "        - tca_data_table (str): Name of the TCS data table.\n",
    "        - prod_com_table (str): Name of the production completion table.\n",
    "        - basin_of_interest (str): Basin of interest for filtering data.\n",
    "        \"\"\"\n",
    "        self.merge_production_table = merge_production_table\n",
    "        self.merge_production_table_columns = merge_production_table_columns\n",
    "        self.pdp_df1_api_list = pdp_df1_api_list\n",
    "        self.basin_of_interest = basin_of_interest\n",
    "        self.analog_well_table = analog_well_table\n",
    "\n",
    "    def download_producing_data(self, spark, cutoff_date, current_date) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Download producing data from QCAST forecast table.\n",
    "\n",
    "        Parameters:\n",
    "        - spark: PySpark session.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: Produced DataFrame.\n",
    "        \"\"\"\n",
    "\n",
    "        query = f\"\"\"\n",
    "                SELECT prod.API10, \n",
    "                    prod.Date, \n",
    "                    prod.Water_BBL / 1000 AS Water_MBO, \n",
    "                    prod.Oil_BBL / 1000 AS Oil_MBO, \n",
    "                    prod.Gas_MCF / 1000000 AS Gas_BCF, \n",
    "                    prod.NormalizedMonth as producing_month,\n",
    "                    ana.typeCurveArea,\n",
    "                    ana.FlowUnit_Analog,\n",
    "                    ana.LateralLength_FT,\n",
    "                    ana.ReservoirGoldConsolidated,\n",
    "                    ana.OperatorGold\n",
    "                FROM {self.merge_production_table} prod\n",
    "                INNER JOIN {self.analog_well_table} ana\n",
    "                ON ana.api10 = prod.api10\n",
    "                AND ana.wellStatus = \"PRODUCING\"\n",
    "                WHERE date > '{cutoff_date}' \n",
    "                    AND ana.API10 NOT IN {self.pdp_df1_api_list}\n",
    "            \"\"\"\n",
    "\n",
    "        pdp_df2 = spark.sql(query).toPandas()\n",
    "        return pdp_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cda09e90-4d1e-4396-80cd-db0428277454",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/pandas/utils.py:124: UserWarning:\n\nThe conversion of DecimalType columns is inefficient and may take a long time. Column names: [Water_MBO, Oil_MBO, Gas_BCF] If those columns are not necessary, you may consider dropping them or converting to primitive types before the conversion.\n\n"
     ]
    }
   ],
   "source": [
    "columns = (\"API10\", \"Date\", \"Water_BBL\", \"Oil_BBL\", \"Gas_MCFD\", \"NormalizedMonth\")\n",
    "merge_prod_table = \"produced.merge_production\"\n",
    "analog_well_table = \"analog_wells\"\n",
    "api_list = tuple(pdp_df.API10.unique().tolist())\n",
    "downloader = ProducingWellDownloader2(\n",
    "    merge_prod_table, columns, api_list, analog_well_table, basin_of_interest\n",
    ")\n",
    "pdp_df2 = downloader.download_producing_data(\n",
    "    spark, desired_minimum_date_for_producing_wells, current_date\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8986698d-f711-4393-8329-c2670ab64303",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temp1 = pdp_df2.loc[pdp_df2.groupby(\"API10\")[\"producing_month\"].idxmax()]\n",
    "temp1 = pd.merge(\n",
    "    temp1,\n",
    "    tca_produnction_df[\n",
    "        [\"typeCurveArea\", \"producing_month\", \"Oil_MBO\", \"Gas_BCF\", \"Water_MBO\"]\n",
    "    ],\n",
    "    on=(\"typeCurveArea\", \"producing_month\"),\n",
    "    how=\"left\",\n",
    "    suffixes=(\"_actual\", \"_tca_table\"),\n",
    "    validate=\"many_to_one\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d38807d2-7132-4a47-b674-daf02c8ca1d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temp1[\n",
    "    [\n",
    "        \"Gas_BCF_tca_table\",\n",
    "        \"Gas_BCF_actual\",\n",
    "        \"Water_MBO_actual\",\n",
    "        \"Water_MBO_tca_table\",\n",
    "        \"Oil_MBO_actual\",\n",
    "        \"Oil_MBO_tca_table\",\n",
    "    ]\n",
    "] = temp1[\n",
    "    [\n",
    "        \"Gas_BCF_tca_table\",\n",
    "        \"Gas_BCF_actual\",\n",
    "        \"Water_MBO_actual\",\n",
    "        \"Water_MBO_tca_table\",\n",
    "        \"Oil_MBO_actual\",\n",
    "        \"Oil_MBO_tca_table\",\n",
    "    ]\n",
    "].astype(\n",
    "    float\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40308cf8-2bc5-414f-98f3-030d9e46186d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# getting Error factors for gas, water and oil\n",
    "temp1[\"Gas_error_factor\"] = np.where(\n",
    "    temp1[\"Gas_BCF_tca_table\"] != 0,\n",
    "    temp1[\"Gas_BCF_actual\"] / temp1[\"Gas_BCF_tca_table\"],\n",
    "    None,\n",
    ")\n",
    "temp1[\"Water_error_factor\"] = np.where(\n",
    "    temp1[\"Water_MBO_tca_table\"] != 0,\n",
    "    temp1[\"Water_MBO_actual\"] / temp1[\"Water_MBO_tca_table\"],\n",
    "    None,\n",
    ")\n",
    "temp1[\"Oil_error_factor\"] = np.where(\n",
    "    temp1[\"Oil_MBO_tca_table\"] != 0,\n",
    "    temp1[\"Oil_MBO_actual\"] / temp1[\"Oil_MBO_tca_table\"],\n",
    "    None,\n",
    ")\n",
    "\n",
    "\n",
    "# getting first prod date for each api\n",
    "temp1[\"Date\"] = pd.to_datetime(temp1[\"Date\"])\n",
    "temp1[\"first_prod_date\"] = temp1.apply(\n",
    "    lambda row: row[\"Date\"] - pd.DateOffset(months=row[\"producing_month\"] - 1), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82a4aa40-cfd7-4115-afd9-4a75d333cc0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter out APIs with first_prod_date greater than or equal to current_date\n",
    "current_date = pd.to_datetime(current_date)\n",
    "# Ensure first_prod_date is in datetime format\n",
    "temp1[\"first_prod_date\"] = pd.to_datetime(temp1[\"first_prod_date\"])\n",
    "\n",
    "api_to_remove = temp1[temp1.first_prod_date >= current_date][\"API10\"].unique()\n",
    "temp1 = temp1[~temp1.API10.isin(api_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b083d8a-6e53-4070-b62c-7d39dc4d90c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temp1 = temp1.drop(\n",
    "    columns=[\n",
    "        \"Water_MBO_actual\",\n",
    "        \"Oil_MBO_actual\",\n",
    "        \"Gas_BCF_actual\",\n",
    "        \"producing_month\",\n",
    "        \"Oil_MBO_tca_table\",\n",
    "        \"Gas_BCF_tca_table\",\n",
    "        \"Water_MBO_tca_table\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# getting production values from typecurve production table\n",
    "temp2 = pd.merge(\n",
    "    temp1,\n",
    "    tca_produnction_df[\n",
    "        [\"typeCurveArea\", \"producing_month\", \"Oil_MBO\", \"Gas_BCF\", \"Water_MBO\"]\n",
    "    ],\n",
    "    on=\"typeCurveArea\",\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4f7abe0-66d5-4432-bf84-312ae41812b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# correcting values of gas, oil and water based on error factor\n",
    "\n",
    "temp2[[\"Oil_MBO\", \"Gas_BCF\", \"Water_MBO\"]] = temp2[\n",
    "    [\"Oil_MBO\", \"Gas_BCF\", \"Water_MBO\"]\n",
    "].astype(float)\n",
    "\n",
    "\n",
    "# Multiply 'Oil_MBO' with 'Oil_error_factor' only when 'Oil_error_factor' is not None\n",
    "temp2[\"Oil_MBO\"] = temp2[\"Oil_MBO\"] * temp2[\"Oil_error_factor\"].where(\n",
    "    temp2[\"Oil_error_factor\"].notna(), temp2[\"Oil_MBO\"]\n",
    ")\n",
    "\n",
    "# Multiply 'Water_MBO' with 'Water_error_factor' only when 'Water_error_factor' is not None\n",
    "temp2[\"Water_MBO\"] = temp2[\"Water_MBO\"] * temp2[\"Water_error_factor\"].where(\n",
    "    temp2[\"Water_error_factor\"].notna(), temp2[\"Water_MBO\"]\n",
    ")\n",
    "\n",
    "# Multiply 'Gas_BCF' with 'Gas_error_factor' only when 'Gas_error_factor' is not None\n",
    "temp2[\"Gas_BCF\"] = temp2[\"Gas_BCF\"] * temp2[\"Gas_error_factor\"].where(\n",
    "    temp2[\"Gas_error_factor\"].notna(), temp2[\"Gas_BCF\"]\n",
    ")\n",
    "\n",
    "# Removing rows which has producing_month as null\n",
    "temp2 = temp2[temp2.producing_month.notnull()]\n",
    "\n",
    "# getting production date for each month from first prod date and producing month\n",
    "temp2[\"producing_month\"] = temp2[\"producing_month\"].astype(int)\n",
    "temp2[\"production_date\"] = (\n",
    "    (temp2[\"first_prod_date\"].dt.to_period(\"M\")) + temp2[\"producing_month\"] - 1\n",
    ").dt.to_timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49451a33-3d7b-4994-bf40-40bcf46fc8bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/pandas/core/ops/array_ops.py:73: FutureWarning:\n\nComparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n\n"
     ]
    }
   ],
   "source": [
    "pdp_df2.rename({\"Date\": \"production_date\"}, inplace=True, axis=1)\n",
    "\n",
    "temp3 = pd.merge(\n",
    "    temp2,\n",
    "    pdp_df2[[\"API10\", \"production_date\", \"Water_MBO\", \"Oil_MBO\", \"Gas_BCF\"]],\n",
    "    on=(\"API10\", \"production_date\"),\n",
    "    how=\"left\",\n",
    "    validate=\"one_to_one\",\n",
    "    suffixes=(\"_predicted\", \"\"),\n",
    ")\n",
    "\n",
    "temp3[\"Water_MBO\"] = temp3[\"Water_MBO\"].fillna(temp3[\"Water_MBO_predicted\"])\n",
    "temp3[\"Oil_MBO\"] = temp3[\"Oil_MBO\"].fillna(temp3[\"Oil_MBO_predicted\"])\n",
    "temp3[\"Gas_BCF\"] = temp3[\"Gas_BCF\"].fillna(temp3[\"Gas_BCF_predicted\"])\n",
    "temp3.rename({\"production_date\": \"date\"}, inplace=True, axis=1)\n",
    "temp3 = temp3[pdp_df.columns]\n",
    "temp3 = temp3[temp3.date >= desired_minimum_date_for_producing_wells]\n",
    "\n",
    "# removing future api\n",
    "temp3[\"date\"] = temp3[\"date\"].dt.date\n",
    "api_to_remove = temp3[(temp3.producing_month == 1) & (temp3.date >= current_date)][\n",
    "    \"API10\"\n",
    "].unique()\n",
    "temp3 = temp3[~temp3.API10.isin(api_to_remove)]\n",
    "\n",
    "pdp_df = pd.concat([pdp_df, temp3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbe74e4f-9dba-4013-87d5-cb4703e4a666",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdp_df = pdp_df[~pdp_df.API10.isin(wip_well_data.API10.unique())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "303f672b-b8ef-4618-b578-403a1ee9dfe6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# New Wells "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ec6178a-6216-4056-b5c6-d7a45f20c830",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class NewWellDownloader:\n",
    "    \"\"\"\n",
    "    Class for downloading well data and TCA data using PySpark.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        analog_well_table: str,\n",
    "        merge_production: str,\n",
    "        basin_of_interest: str,\n",
    "        flowunit_of_interest: str,\n",
    "        producing_wells,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the NewWellDownloader.\n",
    "\n",
    "        Parameters:\n",
    "        - well_data_table (str): Name of the well data table.\n",
    "        - tca_data_table (str): Name of the TCA data table.\n",
    "        - basin_of_interest (str): Basin of interest for filtering data.\n",
    "        \"\"\"\n",
    "        self.analog_well_table = analog_well_table\n",
    "        self.merge_production = merge_production\n",
    "        self.basin_of_interest = basin_of_interest\n",
    "        self.flowunit_of_interest = flowunit_of_interest\n",
    "        self.producing_wells = producing_wells\n",
    "\n",
    "    def download_well_data(\n",
    "        self, spark, current_date, cutoff_date_for_new_wells\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Download well data based on specified criteria.\n",
    "\n",
    "        Parameters:\n",
    "        - spark: PySpark session.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: Produced DataFrame.\n",
    "        \"\"\"\n",
    "        well_data_df = spark.sql(\n",
    "            f\"\"\"\n",
    "            SELECT\n",
    "                ana.API10,\n",
    "                ana.HoleDirection,\n",
    "                ana.LateralLength_FT,\n",
    "                ana.BasinQuantum,\n",
    "                ana.OperatorGold,\n",
    "                cast(TRUNC(ana.FirstProdDate, 'month') as DATE) as FirstProdDate,\n",
    "                ana.typeCurveArea,\n",
    "                ana.FlowUnit_Analog,\n",
    "                ana.ReservoirGoldConsolidated,\n",
    "                prod.Gas_MCF/1000000 AS Gas_BCF,\n",
    "                prod.Oil_BBL AS Oil_MBO,\n",
    "                prod.Water_BBL AS Water_MBO,\n",
    "                prod.NormalizedMonth AS producing_month,\n",
    "                prod.Date,\n",
    "                DATE_FORMAT(prod.Date, 'yyyy-MM') as newwell_production_month_year\n",
    "            FROM\n",
    "            {self.analog_well_table} ana\n",
    "            LEFT JOIN {self.merge_production} prod \n",
    "                ON ana.api10 = prod.api10\n",
    "            WHERE ana.WellStatus in (\"PRODUCING\")\n",
    "            AND ana.api10 not in {self.producing_wells}\n",
    "                AND ana.HoleDirection = \"H\"\n",
    "                AND ana.FirstProdDate <= '{current_date}'\n",
    "                AND ana.FirstProdDate >= '{cutoff_date_for_new_wells}'\n",
    "            \"\"\"\n",
    "        )\n",
    "        return well_data_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3edcfb92-e971-484a-9588-7d5fb6aafb73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/pandas/utils.py:124: UserWarning:\n\nThe conversion of DecimalType columns is inefficient and may take a long time. Column names: [Gas_BCF, Oil_MBO, Water_MBO] If those columns are not necessary, you may consider dropping them or converting to primitive types before the conversion.\n\n"
     ]
    }
   ],
   "source": [
    "new_wells_table = \"analog_wells\"\n",
    "supporting_table = \"produced.merge_production\"\n",
    "producing_wells_api = tuple(pdp_df.API10.unique())\n",
    "newwelldownload = NewWellDownloader(\n",
    "    new_wells_table,\n",
    "    supporting_table,\n",
    "    basin_of_interest,\n",
    "    flowunit_of_interest,\n",
    "    producing_wells_api,\n",
    ")\n",
    "new_wells_production = newwelldownload.download_well_data(\n",
    "    spark, current_date, cutoff_date_for_new_wells\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b70c429-4b0d-47fe-81e0-a2db44d577ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# filtering api for which we have production data\n",
    "new_wells_production1 = new_wells_production[\n",
    "    new_wells_production.producing_month.notnull()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97aa94b2-d88c-466d-a641-d75adb3b758d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# filtering api for which we do not have production data\n",
    "new_wells_production2 = new_wells_production[\n",
    "    new_wells_production.producing_month.isnull()\n",
    "][\n",
    "    [\n",
    "        \"API10\",\n",
    "        \"LateralLength_FT\",\n",
    "        \"BasinQuantum\",\n",
    "        \"OperatorGold\",\n",
    "        \"FirstProdDate\",\n",
    "        \"typeCurveArea\",\n",
    "        \"FlowUnit_Analog\",\n",
    "        \"ReservoirGoldConsolidated\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "# merging inventory data with tca prod data\n",
    "new_wells_production2 = pd.merge(\n",
    "    new_wells_production2,\n",
    "    tca_produnction_df[\n",
    "        [\"typeCurveArea\", \"producing_month\", \"Oil_MBO\", \"Gas_BCF\", \"Water_MBO\"]\n",
    "    ],\n",
    "    on=\"typeCurveArea\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "new_wells_production2 = new_wells_production2[\n",
    "    (new_wells_production2.producing_month.notnull())\n",
    "    & (new_wells_production2.FirstProdDate.notnull())\n",
    "]\n",
    "\n",
    "# converting inventory_firstprod_date to first day of month\n",
    "new_wells_production2[\"FirstProdDate\"] = (\n",
    "    pd.to_datetime(new_wells_production2[\"FirstProdDate\"])\n",
    "    .dt.to_period(\"M\")\n",
    "    .dt.to_timestamp()\n",
    ")\n",
    "\n",
    "new_wells_production2[\"producing_month\"] = new_wells_production2[\n",
    "    \"producing_month\"\n",
    "].astype(int)\n",
    "# calculate production date based on inventory first production date and producing month\n",
    "\n",
    "new_wells_production2[\"production_date\"] = (\n",
    "    (new_wells_production2[\"FirstProdDate\"].dt.to_period(\"M\"))\n",
    "    + new_wells_production2[\"producing_month\"]\n",
    "    - 1\n",
    ").dt.to_timestamp()\n",
    "\n",
    "new_wells_production2.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9864c0f5-4655-4d17-8ee1-796863b138ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temp1 = new_wells_production1.loc[\n",
    "    new_wells_production1.groupby(\"API10\")[\"producing_month\"].idxmax()\n",
    "]\n",
    "temp1 = pd.merge(\n",
    "    temp1,\n",
    "    tca_produnction_df[\n",
    "        [\"typeCurveArea\", \"producing_month\", \"Oil_MBO\", \"Gas_BCF\", \"Water_MBO\"]\n",
    "    ],\n",
    "    on=(\"typeCurveArea\", \"producing_month\"),\n",
    "    how=\"left\",\n",
    "    suffixes=(\"_actual\", \"_tca_table\"),\n",
    "    validate=\"many_to_one\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7bd5637-bc71-4de8-afea-add358a64841",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temp1[\n",
    "    [\n",
    "        \"Gas_BCF_tca_table\",\n",
    "        \"Gas_BCF_actual\",\n",
    "        \"Water_MBO_actual\",\n",
    "        \"Water_MBO_tca_table\",\n",
    "        \"Oil_MBO_actual\",\n",
    "        \"Oil_MBO_tca_table\",\n",
    "    ]\n",
    "] = temp1[\n",
    "    [\n",
    "        \"Gas_BCF_tca_table\",\n",
    "        \"Gas_BCF_actual\",\n",
    "        \"Water_MBO_actual\",\n",
    "        \"Water_MBO_tca_table\",\n",
    "        \"Oil_MBO_actual\",\n",
    "        \"Oil_MBO_tca_table\",\n",
    "    ]\n",
    "].astype(\n",
    "    float\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db81cfc0-61ec-44c4-90d6-829d245a341f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# getting Error factors for gas, water and oil\n",
    "temp1[\"Gas_error_factor\"] = np.where(\n",
    "    temp1[\"Gas_BCF_tca_table\"] != 0,\n",
    "    temp1[\"Gas_BCF_actual\"] / temp1[\"Gas_BCF_tca_table\"],\n",
    "    None,\n",
    ")\n",
    "temp1[\"Water_error_factor\"] = np.where(\n",
    "    temp1[\"Water_MBO_tca_table\"] != 0,\n",
    "    temp1[\"Water_MBO_actual\"] / temp1[\"Water_MBO_tca_table\"],\n",
    "    None,\n",
    ")\n",
    "temp1[\"Oil_error_factor\"] = np.where(\n",
    "    temp1[\"Oil_MBO_tca_table\"] != 0,\n",
    "    temp1[\"Oil_MBO_actual\"] / temp1[\"Oil_MBO_tca_table\"],\n",
    "    None,\n",
    ")\n",
    "\n",
    "\n",
    "# getting first prod date for each api\n",
    "temp1[\"Date\"] = pd.to_datetime(temp1[\"Date\"])\n",
    "temp1[\"first_prod_date\"] = temp1.apply(\n",
    "    lambda row: row[\"Date\"] - pd.DateOffset(months=row[\"producing_month\"] - 1), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1649d5e2-ec5e-40f8-8344-170d78143075",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter out APIs with first_prod_date greater than or equal to current_date\n",
    "current_date = pd.to_datetime(current_date)\n",
    "# Ensure first_prod_date is in datetime format\n",
    "temp1[\"first_prod_date\"] = pd.to_datetime(temp1[\"first_prod_date\"])\n",
    "\n",
    "api_to_remove = temp1[temp1.first_prod_date >= current_date][\"API10\"].unique()\n",
    "temp1 = temp1[~temp1.API10.isin(api_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afe45269-750f-49e4-b487-723420f1c085",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temp1 = temp1.drop(\n",
    "    columns=[\n",
    "        \"Water_MBO_actual\",\n",
    "        \"Oil_MBO_actual\",\n",
    "        \"Gas_BCF_actual\",\n",
    "        \"producing_month\",\n",
    "        \"Oil_MBO_tca_table\",\n",
    "        \"Gas_BCF_tca_table\",\n",
    "        \"Water_MBO_tca_table\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# getting production values from typecurve production table\n",
    "temp2 = pd.merge(\n",
    "    temp1,\n",
    "    tca_produnction_df[\n",
    "        [\"typeCurveArea\", \"producing_month\", \"Oil_MBO\", \"Gas_BCF\", \"Water_MBO\"]\n",
    "    ],\n",
    "    on=\"typeCurveArea\",\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d7ad465-6796-46f8-8c7e-e7fda1ce9195",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# correcting values of gas, oil and water based on error factor\n",
    "\n",
    "temp2[[\"Oil_MBO\", \"Gas_BCF\", \"Water_MBO\"]] = temp2[\n",
    "    [\"Oil_MBO\", \"Gas_BCF\", \"Water_MBO\"]\n",
    "].astype(float)\n",
    "\n",
    "\n",
    "# Multiply 'Oil_MBO' with 'Oil_error_factor' only when 'Oil_error_factor' is not None\n",
    "temp2[\"Oil_MBO\"] = temp2[\"Oil_MBO\"] * temp2[\"Oil_error_factor\"].where(\n",
    "    temp2[\"Oil_error_factor\"].notna(), temp2[\"Oil_MBO\"]\n",
    ")\n",
    "\n",
    "# Multiply 'Water_MBO' with 'Water_error_factor' only when 'Water_error_factor' is not None\n",
    "temp2[\"Water_MBO\"] = temp2[\"Water_MBO\"] * temp2[\"Water_error_factor\"].where(\n",
    "    temp2[\"Water_error_factor\"].notna(), temp2[\"Water_MBO\"]\n",
    ")\n",
    "\n",
    "# Multiply 'Gas_BCF' with 'Gas_error_factor' only when 'Gas_error_factor' is not None\n",
    "temp2[\"Gas_BCF\"] = temp2[\"Gas_BCF\"] * temp2[\"Gas_error_factor\"].where(\n",
    "    temp2[\"Gas_error_factor\"].notna(), temp2[\"Gas_BCF\"]\n",
    ")\n",
    "\n",
    "# Removing rows which has producing_month as null\n",
    "temp2 = temp2[temp2.producing_month.notnull()]\n",
    "\n",
    "# getting production date for each month from first prod date and producing month\n",
    "temp2[\"producing_month\"] = temp2[\"producing_month\"].astype(int)\n",
    "temp2[\"production_date\"] = (\n",
    "    (temp2[\"first_prod_date\"].dt.to_period(\"M\")) + temp2[\"producing_month\"] - 1\n",
    ").dt.to_timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2a2d6ea-bd83-4bd6-9a0b-e8d908c1b2fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<command-3066998344486554>:1: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/databricks/python/lib/python3.9/site-packages/pandas/core/ops/array_ops.py:73: FutureWarning:\n\nComparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n\n"
     ]
    }
   ],
   "source": [
    "new_wells_production1.rename({\"Date\": \"production_date\"}, inplace=True, axis=1)\n",
    "\n",
    "temp3 = pd.merge(\n",
    "    temp2,\n",
    "    new_wells_production1[\n",
    "        [\"API10\", \"production_date\", \"Water_MBO\", \"Oil_MBO\", \"Gas_BCF\"]\n",
    "    ],\n",
    "    on=(\"API10\", \"production_date\"),\n",
    "    how=\"left\",\n",
    "    validate=\"one_to_one\",\n",
    "    suffixes=(\"_predicted\", \"\"),\n",
    ")\n",
    "\n",
    "temp3[\"Water_MBO\"] = temp3[\"Water_MBO\"].fillna(temp3[\"Water_MBO_predicted\"])\n",
    "temp3[\"Oil_MBO\"] = temp3[\"Oil_MBO\"].fillna(temp3[\"Oil_MBO_predicted\"])\n",
    "temp3[\"Gas_BCF\"] = temp3[\"Gas_BCF\"].fillna(temp3[\"Gas_BCF_predicted\"])\n",
    "temp3.rename({\"production_date\": \"date\"}, inplace=True, axis=1)\n",
    "temp3 = temp3[pdp_df.columns]\n",
    "temp3 = temp3[temp3.date >= desired_minimum_date_for_producing_wells]\n",
    "\n",
    "# removing future api\n",
    "temp3[\"date\"] = temp3[\"date\"].dt.date\n",
    "api_to_remove = temp3[(temp3.producing_month == 1) & (temp3.date >= current_date)][\n",
    "    \"API10\"\n",
    "].unique()\n",
    "temp3 = temp3[~temp3.API10.isin(api_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2844851a-daf5-42fe-9e27-f0664e3e84f7",
     "showTitle": true,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# concatinating both new wells dataframe\n",
    "\n",
    "new_wells_production = pd.concat([new_wells_production2, temp3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75f23063-deca-44f6-91de-e29dd1acb593",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Completed and Drilled Wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "472b7210-a2e4-4cb1-952a-359507120a95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Completed_drilled_wells = typecurve_df[\n",
    "    (typecurve_df.WellStatus.isin([\"COMPLETED\", \"DRILLED\"]))\n",
    "    & (~typecurve_df.API10.isin(pdp_df.API10.unique()))\n",
    "    & (~typecurve_df.API10.isin(new_wells_production.API10.unique()))\n",
    "    & (~typecurve_df.API10.isin(wip_well_data.API10.unique()))\n",
    "    & (typecurve_df.spudDate.notnull())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7018b7c4-9a35-46b3-88ef-aa73d633ea81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<command-3066998344486578>:2: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n"
     ]
    }
   ],
   "source": [
    "# Converting columns to appropriate datatypes\n",
    "Completed_drilled_wells[\"spudDate\"] = pd.to_datetime(\n",
    "    Completed_drilled_wells[\"spudDate\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9098f7fd-2edb-4d2d-a8bc-dcf5c2bd00d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "desired_minimum_firstproddate_for_completed_drilled_wells = pd.to_datetime(\n",
    "    desired_minimum_firstproddate_for_completed_drilled_wells\n",
    ")\n",
    "\n",
    "# Filter the DataFrame\n",
    "Completed_drilled_wells = Completed_drilled_wells[\n",
    "    Completed_drilled_wells.spudDate\n",
    "    >= desired_minimum_firstproddate_for_completed_drilled_wells\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33f0cc1f-3395-4054-b613-56483d1b4e33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "null_indices = Completed_drilled_wells.index[\n",
    "    Completed_drilled_wells[\"FirstProdDate\"].isnull()\n",
    "]\n",
    "\n",
    "for i, idx in enumerate(null_indices):\n",
    "    # Calculate the month offset based on the pattern\n",
    "\n",
    "    month_offset = i // 2\n",
    "    new_date = current_date + relativedelta(months=month_offset)\n",
    "    Completed_drilled_wells.at[idx, \"FirstProdDate\"] = new_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3adebad2-4d67-4eaa-a7c9-a86eaecfbb34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/pandas/core/indexes/base.py:2237: FutureWarning:\n\nComparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n\n/databricks/python/lib/python3.9/site-packages/pandas/core/indexes/base.py:3809: FutureWarning:\n\nComparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n\n"
     ]
    }
   ],
   "source": [
    "# merging completed drilling wells data and tca production data to have\n",
    "Completed_drilled_wells = pd.merge(\n",
    "    Completed_drilled_wells,\n",
    "    tca_produnction_df[\n",
    "        [\"typeCurveArea\", \"producing_month\", \"Oil_MBO\", \"Gas_BCF\", \"Water_MBO\"]\n",
    "    ],\n",
    "    on=\"typeCurveArea\",\n",
    "    how=\"left\",\n",
    "    validate=\"many_to_many\",\n",
    ")\n",
    "\n",
    "Completed_drilled_wells = Completed_drilled_wells[\n",
    "    Completed_drilled_wells.producing_month.notnull()\n",
    "]\n",
    "\n",
    "# getting production month for next 50 years based on the first prod date for each wip well\n",
    "Completed_drilled_wells[\"production_date\"] = (\n",
    "    (pd.to_datetime(Completed_drilled_wells[\"FirstProdDate\"]).dt.to_period(\"M\"))\n",
    "    + Completed_drilled_wells[\"producing_month\"].astype(int)\n",
    "    - 1\n",
    ").dt.to_timestamp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb27cf1c-cba2-4093-ae8d-fc6deecd2d45",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Total Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4346948c-ed30-43c6-89ea-6500b3878637",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def total_production_estimation(\n",
    "    inventory, producing, wip, new, Completed_drilled_wells\n",
    "):\n",
    "    \"\"\"\n",
    "    Combine data from different well statuses into a single DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - inventory (pd.DataFrame): DataFrame for inventory wells.\n",
    "    - producing (pd.DataFrame): DataFrame for producing wells.\n",
    "    - wip (pd.DataFrame): DataFrame for wells in progress.\n",
    "    - new (pd.DataFrame): DataFrame for new wells.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Combined DataFrame.\n",
    "    \"\"\"\n",
    "    # Rename and select columns for inventory wells\n",
    "    inventory.rename(\n",
    "        {\"lateralLength_ft\": \"LateralLength_FT\", \"flowUnit\": \"FlowUnit_Analog\"},\n",
    "        axis=1,\n",
    "        inplace=True,\n",
    "    )\n",
    "    inventory = inventory[\n",
    "        [\n",
    "            \"entityID\",\n",
    "            \"LateralLength_FT\",\n",
    "            \"typeCurveArea\",\n",
    "            \"FlowUnit_Analog\",\n",
    "            \"OperatorGold\",\n",
    "            \"ReservoirGoldConsolidated\",\n",
    "            \"production_date\",\n",
    "            \"producing_month\",\n",
    "            \"Oil_MBO\",\n",
    "            \"Gas_BCF\",\n",
    "            \"Water_MBO\",\n",
    "        ]\n",
    "    ]\n",
    "    inventory[\"forecastType\"] = \"inventory_wells\"\n",
    "\n",
    "    # Rename and select columns for producing wells\n",
    "    producing.rename(\n",
    "        {\"API10\": \"entityID\", \"date\": \"production_date\"}, axis=1, inplace=True\n",
    "    )\n",
    "    producing = producing[\n",
    "        [\n",
    "            \"entityID\",\n",
    "            \"LateralLength_FT\",\n",
    "            \"typeCurveArea\",\n",
    "            \"FlowUnit_Analog\",\n",
    "            \"OperatorGold\",\n",
    "            \"ReservoirGoldConsolidated\",\n",
    "            \"production_date\",\n",
    "            \"producing_month\",\n",
    "            \"Oil_MBO\",\n",
    "            \"Gas_BCF\",\n",
    "            \"Water_MBO\",\n",
    "        ]\n",
    "    ]\n",
    "    producing[\"forecastType\"] = \"producing_wells\"\n",
    "\n",
    "    # Rename and select columns for wells in progress\n",
    "    wip.rename(\n",
    "        {\"API10\": \"entityID\", \"lateralLength_Ft\": \"LateralLength_FT\"},\n",
    "        axis=1,\n",
    "        inplace=True,\n",
    "    )\n",
    "    wip = wip[\n",
    "        [\n",
    "            \"entityID\",\n",
    "            \"LateralLength_FT\",\n",
    "            \"typeCurveArea\",\n",
    "            \"FlowUnit_Analog\",\n",
    "            \"OperatorGold\",\n",
    "            \"ReservoirGoldConsolidated\",\n",
    "            \"production_date\",\n",
    "            \"producing_month\",\n",
    "            \"Oil_MBO\",\n",
    "            \"Gas_BCF\",\n",
    "            \"Water_MBO\",\n",
    "        ]\n",
    "    ]\n",
    "    wip[\"forecastType\"] = \"wip_wells\"\n",
    "\n",
    "    # Rename and select columns for new wells\n",
    "    new.rename({\"API10\": \"entityID\"}, axis=1, inplace=True)\n",
    "    new = new[\n",
    "        [\n",
    "            \"entityID\",\n",
    "            \"LateralLength_FT\",\n",
    "            \"typeCurveArea\",\n",
    "            \"FlowUnit_Analog\",\n",
    "            \"OperatorGold\",\n",
    "            \"ReservoirGoldConsolidated\",\n",
    "            \"production_date\",\n",
    "            \"producing_month\",\n",
    "            \"Oil_MBO\",\n",
    "            \"Gas_BCF\",\n",
    "            \"Water_MBO\",\n",
    "        ]\n",
    "    ]\n",
    "    new[\"forecastType\"] = \"new_wells\"\n",
    "\n",
    "    Completed_drilled_wells.rename({\"API10\": \"entityID\"}, axis=1, inplace=True)\n",
    "    Completed_drilled_wells = Completed_drilled_wells[\n",
    "        [\n",
    "            \"entityID\",\n",
    "            \"LateralLength_FT\",\n",
    "            \"typeCurveArea\",\n",
    "            \"FlowUnit_Analog\",\n",
    "            \"OperatorGold\",\n",
    "            \"ReservoirGoldConsolidated\",\n",
    "            \"production_date\",\n",
    "            \"producing_month\",\n",
    "            \"Oil_MBO\",\n",
    "            \"Gas_BCF\",\n",
    "            \"Water_MBO\",\n",
    "        ]\n",
    "    ]\n",
    "    Completed_drilled_wells[\"forecastType\"] = \"drilled_completed\"\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    final_df = pd.concat(\n",
    "        [inventory, wip, new, producing, Completed_drilled_wells], ignore_index=True\n",
    "    )\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6052cecc-951b-4216-9341-225eb693aa43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<command-719240530684973>:18: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n<command-719240530684973>:25: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n<command-719240530684973>:32: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n<command-719240530684973>:38: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n<command-719240530684973>:43: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/databricks/python/lib/python3.9/site-packages/pandas/core/ops/array_ops.py:73: FutureWarning:\n\nComparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n\n"
     ]
    }
   ],
   "source": [
    "final_df = total_production_estimation(\n",
    "    inventory_production,\n",
    "    pdp_df,\n",
    "    wip_well_production,\n",
    "    new_wells_production,\n",
    "    Completed_drilled_wells,\n",
    ")\n",
    "\n",
    "final_df = final_df[\n",
    "    final_df.production_date <= pd.Timestamp(desired_maximum_date_for_producing_wells)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ca9c6f1-b8cb-4a19-b78f-89b103af5991",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Normalization of oil and gas values based on Lateral length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0771caf4-6452-460e-acbb-e86d05c1e623",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def denormalize_vectorized(\n",
    "    eur_normalized, lateral_length_ft, qcast_nom_lateral_ft, error_factor\n",
    "):\n",
    "    \"\"\"\n",
    "    Vectorized denormalization for predictions based on lateral length, QCAST nominal lateral length, and error factor.\n",
    "\n",
    "    Parameters:\n",
    "    - eur_normalized (pd.Series): Normalized prediction values.\n",
    "    - lateral_length_ft (pd.Series): Actual lateral lengths.\n",
    "    - qcast_nom_lateral_ft (float): QCAST nominal lateral length.\n",
    "    - error_factor (float): Error factor for denormalization.\n",
    "\n",
    "    Returns:\n",
    "    - pd.Series: Denormalized predictions.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        eur_normalized\n",
    "        * lateral_length_ft\n",
    "        / (\n",
    "            lateral_length_ft\n",
    "            + (qcast_nom_lateral_ft - lateral_length_ft) * error_factor\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# Assume 'final_df' is your DataFrame and it has the necessary columns\n",
    "qcast_nom_lateral_ft = 10000\n",
    "error_factor = 0.8\n",
    "columns_to_denormalize = [\"Oil_MBO\", \"Gas_BCF\", \"Water_MBO\"]\n",
    "\n",
    "# Vectorized denormalization for each column\n",
    "for column in columns_to_denormalize:\n",
    "    final_df[column] = denormalize_vectorized(\n",
    "        final_df[column].astype(float),\n",
    "        final_df[\"LateralLength_FT\"],\n",
    "        qcast_nom_lateral_ft,\n",
    "        error_factor,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9658b2d5-4795-4b30-b947-18411bd36edb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/pandas/core/indexes/base.py:2237: FutureWarning:\n\nComparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n\n/databricks/python/lib/python3.9/site-packages/pandas/core/indexes/base.py:3809: FutureWarning:\n\nComparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n\n"
     ]
    }
   ],
   "source": [
    "final_df[\"production_date\"] = pd.to_datetime(final_df[\"production_date\"])\n",
    "final_df[\"production_month_year\"] = final_df[\"production_date\"].dt.to_period(\"M\")\n",
    "\n",
    "final_df[\"Gas_BCFD\"] = final_df[\"Gas_BCF\"] / 30.4375\n",
    "\n",
    "final_df[\"producing_month\"] = final_df[\"producing_month\"].astype(int)\n",
    "final_df[\"production_month_year\"] = final_df[\"production_month_year\"].astype(str)\n",
    "\n",
    "final_df[\"basin\"] = basin_of_interest\n",
    "final_df[\"scenario_id\"] = scenario_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acb75fe2-3d0f-46f1-818d-7a71ed7a5efe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Creating Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a4916bd-851c-4f9a-bb8e-8836f0b84135",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[671]: DataFrame[num_affected_rows: bigint]"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "          delete from produced.production_estimation_model where scenario_id = \"{scenario_id}\"\n",
    "          \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2fc6d33-63d4-4478-be93-3483453a46c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    spark.createDataFrame(final_df)\n",
    "    .withColumn(\"LateralLength_FT\", col(\"LateralLength_FT\").cast(FloatType()))\n",
    "    .withColumn(\"producing_month\", col(\"producing_month\").cast(IntegerType()))\n",
    "    .withColumn(\"Oil_MBO\", col(\"Oil_MBO\").cast(FloatType()))\n",
    "    .withColumn(\"Gas_BCF\", col(\"Gas_BCF\").cast(FloatType()))\n",
    "    .withColumn(\"Water_MBO\", col(\"Water_MBO\").cast(FloatType()))\n",
    "    .withColumn(\"Gas_BCFD\", col(\"Gas_BCFD\").cast(FloatType()))\n",
    "    .write.format(\"delta\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .mode(\"append\")\n",
    "    .saveAsTable(f\"produced.production_estimation_model\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b73ac374-07cb-495f-bc4b-cb1048607619",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3463741269671665,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04-PRODUCTION-ESTIMATION-MODEL",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
